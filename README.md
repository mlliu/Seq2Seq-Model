# n-gram Language Model 

In the first assignment,we implement a trigram language model and train it on a small corpus. we then implement a scoring function to compute the perplexity of the model on a held-out test set. Finally, we implement some methods to deal with sparsity (zero count) issues in your model.

#CKY algorithm

In the lecture on Syntax, we learnt about parsing algorithms, including the bottom-up CYK algorithm. In this section, we implemented the CYK algorithm for computing the parse tree of a sentence given a grammar.

# RNN

Part 1: Language modeling with RNNs: For the first part of this assignment, we train a character-level language model on a lyrics dataset. More specifically, we will Implement a character-level language model based on recurrent neural networks.
Train it on a lyrics dataset. Sample previously unseen lyrics from our model. 

#Word2Vec

In Part 2 of this assignment, we will use Word2Vec to analyze some of interesting phenomena that arises from distributional semantics. For this, we will use a pretrained model from the Gensim package.

# Seq2Seq Model

In this assignment, you will build sequence-to-sequence models for pronunciation prediction of English words, which simply means that given a word (sequence of characters), the model should predict its pronunciation (sequence of phonemes). You should be able to see why this is a straightforward application of sequence-to-sequence models.
