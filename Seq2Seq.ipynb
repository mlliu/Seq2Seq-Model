{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNdjoo4DQGgu",
    "outputId": "6fea6845-b5d4-4c03-e166-619beb0592f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-20 22:44:20--  https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw3-files/student/required_files.zip\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/assignments/hw3-files/student/required_files.zip [following]\n",
      "--2022-11-20 22:44:20--  https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/assignments/hw3-files/student/required_files.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3671 (3.6K) [application/zip]\n",
      "Saving to: ‘required_files.zip’\n",
      "\n",
      "\r",
      "required_files.zip    0%[                    ]       0  --.-KB/s               \r",
      "required_files.zip  100%[===================>]   3.58K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-20 22:44:20 (59.7 MB/s) - ‘required_files.zip’ saved [3671/3671]\n",
      "\n",
      "Archive:  required_files.zip\n",
      "  inflating: requirements.txt        \n",
      "   creating: tests/\n",
      "  inflating: tests/model-generate-test.py  \n",
      "  inflating: tests/model-generate-checks.py  \n",
      "  inflating: tests/metric-cer-impl.py  \n",
      "  inflating: tests/metric-torchmetric-impl.py  \n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: datascience in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.17.5)\n",
      "Requirement already satisfied: jupyter_client in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (6.1.12)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (5.3.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.3.5)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.7.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.11.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (5.6.1)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (5.7.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.3.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.21.6)\n",
      "Collecting otter-grader==4.0.1\n",
      "  Downloading otter_grader-4.0.1-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 25.8 MB/s \n",
      "\u001b[?25hCollecting pdfkit\n",
      "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-2.11.2-py3-none-any.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 57.5 MB/s \n",
      "\u001b[?25hCollecting wkhtmltopdf\n",
      "  Downloading wkhtmltopdf-0.2.tar.gz (9.7 kB)\n",
      "Collecting overrides===6.2.0\n",
      "  Downloading overrides-6.2.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: torch~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.12.1+cu113)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.13.1+cu113)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (0.12.1+cu113)\n",
      "Collecting pytorch-lightning~=1.7.7\n",
      "  Downloading pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
      "\u001b[K     |████████████████████████████████| 708 kB 48.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (1.0.2)\n",
      "Collecting transformers~=4.23.1\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 38.5 MB/s \n",
      "\u001b[?25hCollecting torchmetrics~=0.10.1\n",
      "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 64.2 MB/s \n",
      "\u001b[?25hCollecting jupytext\n",
      "  Downloading jupytext-1.14.1-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 57.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (2.23.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (7.1.2)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (1.12.11)\n",
      "Collecting python-on-whales\n",
      "  Downloading python_on_whales-0.54.0-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 6.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (1.14.1)\n",
      "Collecting fica>=0.2.0\n",
      "  Downloading fica-0.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (0.4.6)\n",
      "Requirement already satisfied: gspread in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.12.1->-r requirements.txt (line 23)) (4.1.1)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2022.10.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (21.3)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2.9.1)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 71.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.23.1->-r requirements.txt (line 31)) (2022.6.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 73.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=4.23.1->-r requirements.txt (line 31)) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers~=4.23.1->-r requirements.txt (line 31)) (4.13.0)\n",
      "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.8.6)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.7/dist-packages (from fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.17.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.8.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.0.9)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.19.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (57.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.38.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.3.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.50.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers~=4.23.1->-r requirements.txt (line 31)) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->otter-grader==4.0.1->-r requirements.txt (line 15)) (3.2.2)\n",
      "Requirement already satisfied: branca in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: folium>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (0.12.1.post1)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (7.9.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (5.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->-r requirements.txt (line 10)) (2.0.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (4.11.2)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.2.0)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 72.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->datascience->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->datascience->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 5)) (2022.6)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (3.6.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (5.7.16)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.15.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (5.0.1)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->-r requirements.txt (line 12)) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->-r requirements.txt (line 12)) (4.3.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 12)) (0.19.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 12)) (5.10.0)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 24)) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 29)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 29)) (1.2.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.17.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.8.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.56.4)\n",
      "Collecting markdown-it-py<3.0.0,>=1.0.0\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from jupytext->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.10.2)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.8 MB/s \n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->datascience->-r requirements.txt (line 1)) (8.1.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from python-on-whales->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.10.2)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from python-on-whales->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.4.2)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.2.4)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.4.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.11.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.7.12)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.1.5)\n",
      "Building wheels for collected packages: sklearn, wkhtmltopdf\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=c3abcdd148e95c4c5375aef0d0ba623dcfaf15ac50e77ed5c1c8eff619562364\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
      "  Building wheel for wkhtmltopdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wkhtmltopdf: filename=wkhtmltopdf-0.2-py3-none-any.whl size=11149 sha256=1220ba9afd0f0c60029adf5d6d61229f57cb6b6ea786ac6089bf2335938d7aab\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/15/f3/c2f2199ba236c73e7e14888e5e5e5ff6da2fb13d1f26072a9e\n",
      "Successfully built sklearn wkhtmltopdf\n",
      "Installing collected packages: jedi, mdurl, markdown-it-py, mdit-py-plugins, torchmetrics, tokenizers, python-on-whales, pyDeprecate, jupytext, huggingface-hub, fica, wkhtmltopdf, transformers, sklearn, pytorch-lightning, PyPDF2, pdfkit, overrides, otter-grader\n",
      "Successfully installed PyPDF2-2.11.2 fica-0.2.2 huggingface-hub-0.11.0 jedi-0.18.1 jupytext-1.14.1 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 otter-grader-4.0.1 overrides-6.2.0 pdfkit-1.0.0 pyDeprecate-0.3.2 python-on-whales-0.54.0 pytorch-lightning-1.7.7 sklearn-0.0.post1 tokenizers-0.13.2 torchmetrics-0.10.3 transformers-4.23.1 wkhtmltopdf-0.2\n"
     ]
    }
   ],
   "source": [
    "# Downloads required packages and files\n",
    "required_files = \"https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw3-files/student/required_files.zip\"\n",
    "! wget $required_files && unzip -o required_files.zip\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XClhNa01QGgx"
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell if you want to run on TPU\n",
    "# Installs XLA for TPU support\n",
    "# ! pip install cloud-tpu-client==0.10 torch-xla https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7sSTMiZdQGgy"
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "\n",
    "grader = otter.Notebook(colab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLm_JxCxrkIE"
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "You have now learnt about sequence-to-sequence models in the context of machine translation. In the upcoming lectures, you will see that these models are useful for a wide variety of tasks, wherever the source sequence and the target sequence have different lengths. You have also been introduced to phonemes, which are the building blocks of speech. \n",
    "\n",
    "In this assignment, you will build sequence-to-sequence models for pronunciation prediction of English words, which simply means that given a word (sequence of characters), the model should predict its pronunciation (sequence of phonemes). You should be able to see why this is a straightforward application of sequence-to-sequence models.\n",
    "\n",
    "The input is a sequence of characters making up an English word e.g. `a l g e b r a i c a l l y`. The output should be a sequence of phonemes that describe the pronunciation. For the example above, the desired output should be `AE2 L JH AH0 B R EY1 IH0 K L IY0`. The data for this task was obtained from the [CMU Pronunciation Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict). We will use a small subset of the CMU dict data.\n",
    "\n",
    "# Setup\n",
    "\n",
    "For this assignment, as in the previous one, we will be using Google Colab, for both code as well as descriptive questions. Your task is to finish all the questions in the Colab notebook and then upload a PDF version of the notebook, and a viewable link on Gradescope. \n",
    "\n",
    "### Google colaboratory\n",
    "\n",
    "Before getting started, get familiar with google colaboratory:\n",
    "https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "\n",
    "This is a neat python environment that works in the cloud and does not require you to\n",
    "set up anything on your personal machine\n",
    "(it also has some built-in IDE features that make writing code easier).\n",
    "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
    "with other people.\n",
    "\n",
    "__Note:__\n",
    "1. You may need to change your Runtime setting to GPU in order to run the following code blocks.\n",
    "2. On changing the Runtime setting, you would be required to run the previous code-blocks again.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Before you start working on this homework do the following steps:\n",
    "\n",
    "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
    "2. Follow all the steps in this collaboratory file and write / change / uncomment code as necessary.\n",
    "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
    "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected. Copy the link and paste it in the box below.\n",
    "5. After completing the notebook, press __File > Download .ipynb__ to download a local copy on your computer, and then upload the file to Gradescope.\n",
    "6. Please export the notebook to PDF and upload the PDF to the writing part.\n",
    "\n",
    "__Special handling for model checkpoints.__\n",
    "6. As the homework requires training neural models, such trained model checkpoints should also be submitted together with the notebook, hence avoiding re-training during the grading phase. For such model checkpoints, they would be stored at `./lightning_logs` directory. You have to first locate the directory from the left side panel (`Files`) on Colab.\n",
    "7. Enter `./lightning_logs` and find the training label that you would like to submit. The versions are labelled with respect to the training calls.\n",
    "8. Download the `.ckpt` file from `./lightning_logs/<your_version>/checkpoints/<name>.ckpt`.\n",
    "9. Rename the downloaded checkpoint and re-name it as the corresponding name shown in the question, say `vanilla_rnn_model.ckpt`.\n",
    "10. Submit checkpoint file(s) together with your notebook to the autograder. Please make sure that checkpoint files should be put at the same directory level as the notebook (content root).\n",
    "\n",
    "\n",
    "\n",
    "__Paste your notebook link in the box below.__ _(0 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2kR0zyxKUh2"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "https://colab.research.google.com/drive/1a2NX3AIkZxP87z6s1ZcNReV0dP3ovV4g?usp=sharing\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NHfmDfX5QGg2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import enum\n",
    "import json\n",
    "from itertools import zip_longest\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple, Any, Union\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IwMXaQxQGg2",
    "outputId": "7ac8cb56-d3b3-400e-9c63-9814f6557efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is running for \"student\".\n",
      "Students should make sure you are running under the \"student\" mode.\n",
      "You are using \"cpu\".\n"
     ]
    }
   ],
   "source": [
    "# Checks whether it is in the autograder grading mode\n",
    "# Checks whether GPU accelerators are available\n",
    "is_autograder = os.path.exists('is_autograder.py')\n",
    "if torch.cuda.is_available() and not is_autograder:\n",
    "    accelerator = 'gpu'\n",
    "elif os.environ.get('COLAB_TPU_ADDR') is not None and not is_autograder:\n",
    "    accelerator = 'tpu'\n",
    "else:\n",
    "    accelerator = 'cpu'\n",
    "print(f'The notebook is running for \"{\"autograder\" if is_autograder else \"student\"}\".')\n",
    "print('Students should make sure you are running under the \"student\" mode.')\n",
    "print(f'You are using \"{accelerator}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPa7j8XyQGg3",
    "outputId": "27afcae5-5e46-4727-8d75-20b7f4ce0ff3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed everything to make sure all experiments are reproducible\n",
    "pl.seed_everything(seed=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WGT_7JFUQGg3"
   },
   "outputs": [],
   "source": [
    "# Defines constants\n",
    "HOMEWORK_DATA_URL = \"https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw3-files/student/\"\n",
    "\n",
    "SRC_SUFFIX = '.src'\n",
    "TGT_SUFFIX = '.tgt'\n",
    "\n",
    "CMUDICT_BASE = 'cmudict'\n",
    "CMUDICT_TRAIN_SMALL = CMUDICT_BASE + '.small.train'\n",
    "CMUDICT_TRAIN = CMUDICT_BASE + '.train'\n",
    "CMUDICT_DEV = CMUDICT_BASE + '.dev'\n",
    "\n",
    "CMUDICT_SRC_VOCAB = 'cmudict.src.vocab.json'\n",
    "CMUDICT_TGT_VOCAB = 'cmudict.tgt.vocab.json'\n",
    "\n",
    "# Special tokens\n",
    "class SpecialToken(enum.Enum):\n",
    "    BOS = '<BOS>'\n",
    "    EOS = '<EOS>'\n",
    "    UNK = '<UNK>'\n",
    "    PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JOXCL90rQGg4"
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    def _download(url: str, filename: str) -> str:\n",
    "        txt = urllib.request.urlopen(url)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(txt.read().decode('utf-8'))\n",
    "\n",
    "    for suffix in (SRC_SUFFIX, TGT_SUFFIX):\n",
    "        _download(f'{HOMEWORK_DATA_URL}/{CMUDICT_TRAIN_SMALL}{suffix}', f'{CMUDICT_TRAIN_SMALL}{suffix}')\n",
    "        _download(f'{HOMEWORK_DATA_URL}/{CMUDICT_TRAIN}{suffix}', f'{CMUDICT_TRAIN}{suffix}')\n",
    "        _download(f'{HOMEWORK_DATA_URL}/{CMUDICT_DEV}{suffix}', f'{CMUDICT_DEV}{suffix}')\n",
    "        _download(f'{HOMEWORK_DATA_URL}/{CMUDICT_SRC_VOCAB}', f'{CMUDICT_SRC_VOCAB}')\n",
    "        _download(f'{HOMEWORK_DATA_URL}/{CMUDICT_TGT_VOCAB}', f'{CMUDICT_TGT_VOCAB}')\n",
    "\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA0rlyDdQGg4"
   },
   "source": [
    "# Vocabulary\n",
    "\n",
    "In the previous homework, we did not specifically define a class to serve as `Vocabulary`. In this homework, as we have to deal with both the source side vocabulary and target side vocabulary, which are different, it is easier to have a class to perform the index-string and string-index mappings.\n",
    "\n",
    "**Although there is nothing for you to implement, we suggest you to walk through the whole implementation to understand each function and think about what they might be used.** If you are not familiar with any concepts here, you should either figure it out from previous homework or post questions on Piazza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VZ8jN0_oQGg5"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self,\n",
    "                 tokens: Optional[List[str]] = None):\n",
    "        # Registers `SpecialToken`\n",
    "        self.special_token_enum = SpecialToken\n",
    "        self._special_tokens = set([s.value for s in SpecialToken])\n",
    "        if tokens is not None:\n",
    "            tokens = set(tokens) | set(self._special_tokens)\n",
    "        else:\n",
    "            tokens = set(self._special_tokens)\n",
    "        self._idx2token = list(tokens)\n",
    "        self._token2idx = {t: i for i, t in enumerate(self._idx2token)}\n",
    "\n",
    "    def add_token(self, token: str) -> int:\n",
    "        if token not in self._idx2token:\n",
    "            self._token2idx[token] = len(self._idx2token)\n",
    "            self._idx2token.append(token)\n",
    "        return self.token2idx(token)\n",
    "\n",
    "    def to_file(self, file_path: str):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'idx2token': self._idx2token,\n",
    "                'special_tokens': list(self._special_tokens),\n",
    "                'token2idx': self._token2idx\n",
    "            }, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file_path: str) -> 'Vocabulary':\n",
    "        with open(file_path) as f:\n",
    "            read_data = json.load(f)\n",
    "        vocab = Vocabulary()\n",
    "        vocab._special_tokens = set(read_data['special_tokens'])\n",
    "        vocab._idx2token = read_data['idx2token']\n",
    "        vocab._token2idx = read_data['token2idx']\n",
    "        return vocab\n",
    "\n",
    "    def is_special(self, token: str) -> bool:\n",
    "        return token in self._special_tokens\n",
    "\n",
    "    def token2idx(self, token: str) -> int:\n",
    "        return self._token2idx.get(token, self._token2idx[str(self.unk().value)])\n",
    "\n",
    "    def idx2token(self, index: int) -> str:\n",
    "        return self._idx2token[index]\n",
    "\n",
    "    def bos(self) -> SpecialToken:\n",
    "        return self.special_token_enum.BOS\n",
    "\n",
    "    def eos(self) -> SpecialToken:\n",
    "        return self.special_token_enum.EOS\n",
    "\n",
    "    def unk(self) -> SpecialToken:\n",
    "        return self.special_token_enum.UNK\n",
    "\n",
    "    def pad(self) -> SpecialToken:\n",
    "        return self.special_token_enum.PAD\n",
    "\n",
    "    def special_to_id(self, st: SpecialToken) -> int:\n",
    "        return self.token2idx(token=str(st.value))\n",
    "\n",
    "    def bos_id(self) -> int:\n",
    "        return self.special_to_id(st=self.bos())\n",
    "\n",
    "    def eos_id(self) -> int:\n",
    "        return self.special_to_id(st=self.eos())\n",
    "\n",
    "    def unk_id(self) -> int:\n",
    "        return self.special_to_id(st=self.unk())\n",
    "\n",
    "    def pad_id(self) -> int:\n",
    "        return self.special_to_id(st=self.pad())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tVMN66zDQGg5"
   },
   "outputs": [],
   "source": [
    "def decode_as_str(tgt_vocab: Vocabulary, output_tensor: torch.Tensor) -> List[str]:\n",
    "    \"\"\"Decodes generation outputs to a list of strings.\"\"\"\n",
    "    outputs: List[List[int]] = output_tensor.detach().tolist()\n",
    "    ignore_token_ids = (tgt_vocab.pad_id(), tgt_vocab.bos_id(), tgt_vocab.eos_id())\n",
    "    decoded_strs: List[str] = [\n",
    "        ' '.join([\n",
    "            tgt_vocab.idx2token(index=tid)  # Converts token id to token\n",
    "            for tid in b\n",
    "            if tid not in ignore_token_ids  # Filters special tokens\n",
    "        ])  # Creates a decoded str\n",
    "        for b in outputs  # Iterates over the batch\n",
    "    ]\n",
    "    return decoded_strs\n",
    "\n",
    "\n",
    "def encode_as_tensor(src_vocab: Vocabulary, sentence: str) -> torch.Tensor:\n",
    "    \"\"\"Encodes a sentence to a tensor via the source vocabulary.\"\"\"\n",
    "    return torch.tensor(\n",
    "        [\n",
    "            [src_vocab.bos_id()]\n",
    "            + [\n",
    "                src_vocab.token2idx(token)\n",
    "                for token in sentence.strip().split()\n",
    "            ]\n",
    "            + [src_vocab.eos_id()]\n",
    "        ],\n",
    "        dtype=torch.long\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6lPT92hOQGg6"
   },
   "outputs": [],
   "source": [
    "# We have pre-built vocabs to ensure deterministic mappings across models\n",
    "# So we can directly load these vocabs from files\n",
    "cmudict_src_vocab = Vocabulary.from_file(CMUDICT_SRC_VOCAB)\n",
    "cmudict_tgt_vocab = Vocabulary.from_file(CMUDICT_TGT_VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noS7yfWbjUzj"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "For your convenience, we have preselected a subset of the CMU pronunciation dictionary (input-output pairs) and split the subset into training, validation (dev) and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUUkEcKMj16Q"
   },
   "source": [
    "# Data Reader\n",
    "\n",
    "We will use the `ParallelDataset` class below to load, process and iterate through the data. Since you worked quite a bit on data loading implementation in the previous assignment, we will provide you the loader for this one, so you can get on with the more interesting parts of this assignment. Why is it parallel? Each source sequence is paired with a target sequence. The task is to generate a target sequence given the source sequence.\n",
    "\n",
    "Note the use of special symbols `<BOS>`, `<EOS>`, `<UNK>`, and `<PAD>`. All the sequences (both input and output) are made to begin with `<BOS>` (Begin of sequence) and end with `<EOS>` (End of sequence). The `<UNK>` symbol is used if we encouter any new symbol that we have not seen (unknown symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ourbVhQaQGg7"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ParallelInstance:\n",
    "    src_seq: torch.Tensor  # Shape: (seq_len)\n",
    "    src_tokens: List[str]\n",
    "    tgt_seq: Optional[torch.Tensor]  # Shape: (seq_len)\n",
    "    tgt_tokens: Optional[List[str]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParallelBatch:\n",
    "    src_seqs: torch.Tensor  # Shape: (batch_size, src_seq_len)\n",
    "    src_attention_mask: torch.Tensor  # Shape: (batch_size, src_seq_len)\n",
    "    src_tokens: List[List[str]]\n",
    "    tgt_seqs: Optional[torch.Tensor]  # Shape: (batch_size, tgt_seq_len)\n",
    "    tgt_attention_mask: Optional[torch.Tensor]  # Shape: (batch_size, src_seq_len)\n",
    "    tgt_tokens: Optional[List[List[str]]]\n",
    "\n",
    "\n",
    "class ParallelDataset(Dataset):\n",
    "    \"\"\"A dataset class that reads the parallel data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_file: str,\n",
    "                 tgt_file: Optional[str] = None,\n",
    "                 src_vocab: Optional[Vocabulary] = None,\n",
    "                 tgt_vocab: Optional[Vocabulary] = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_file : str\n",
    "            Path to the source side file.\n",
    "        tgt_file : str, optional\n",
    "            Path to the target side file. During the testing time, it could be `None`.\n",
    "        src_vocab : Vocabulary, optional\n",
    "            An existing `Vocabulary` object for the source side. If not provided,\n",
    "            one will be created by iterating over source data.\n",
    "        tgt_vocab : Vocabulary, optional\n",
    "            An existing `Vocabulary` object for the target side. If not provided,\n",
    "            one will be created by iterating over target data.\n",
    "        \"\"\"\n",
    "        src_token_sequences = self.read_data(src_file)\n",
    "        tgt_token_sequences = self.read_data(tgt_file) if tgt_file is not None else []\n",
    "        self.src_vocab = src_vocab if src_vocab is not None else self.build_vocab(src_token_sequences)\n",
    "        self.tgt_vocab = tgt_vocab if tgt_vocab is not None else self.build_vocab(tgt_token_sequences)\n",
    "        src_tensors = self.tensorize(vocab=self.src_vocab, sequences=src_token_sequences)\n",
    "        tgt_tensors = self.tensorize(vocab=self.tgt_vocab,\n",
    "                                     sequences=tgt_token_sequences) if tgt_file is not None else []\n",
    "\n",
    "        self.instances: List[ParallelInstance] = [\n",
    "            ParallelInstance(s, st, t, tt)\n",
    "            for s, t, st, tt in zip_longest(\n",
    "                src_tensors, tgt_tensors, src_token_sequences, tgt_token_sequences,\n",
    "                fillvalue=None\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def tensorize(self, vocab: Vocabulary, sequences: List[List[str]]) -> List[torch.Tensor]:\n",
    "        indexed_tensors: List[torch.Tensor] = []\n",
    "        for seq in sequences:\n",
    "            # Shape: (1, seq_len)\n",
    "            new_tensor = torch.tensor(\n",
    "                [vocab.token2idx(t) for t in self.add_special_tokens(vocab, seq)],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            indexed_tensors.append(new_tensor)\n",
    "        return indexed_tensors\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(filepath: str) -> List[List[str]]:\n",
    "        data: List[List[str]] = []\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            for l in f:\n",
    "                d = [tok for tok in l.strip().split()]\n",
    "                data.append(d)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def add_special_tokens(vocab: Vocabulary, seq: List[str]) -> List[str]:\n",
    "        return [str(vocab.bos().value)] + seq + [str(vocab.eos().value)]\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocab(sequences: List[List[str]]) -> Vocabulary:\n",
    "        return Vocabulary(tokens=[t for s in sequences for t in s])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of instances read in the dataset.\"\"\"\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, index: int) -> ParallelInstance:\n",
    "        return self.instances[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4_EvLXSUQGg7"
   },
   "outputs": [],
   "source": [
    "class ParallelDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Wraps PyTorch dataset as a lightning data module.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_paths: Dict[str, str],\n",
    "                 batch_size: int = 32,\n",
    "                 shuffle: bool = True,\n",
    "                 src_vocab: Optional[Vocabulary] = None,\n",
    "                 tgt_vocab: Optional[Vocabulary] = None):\n",
    "        super(ParallelDataModule, self).__init__()\n",
    "\n",
    "        self.datasets: Dict[str, Dataset] = {}\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        for split in ('train', 'val', 'test'):\n",
    "            if split not in dataset_paths:\n",
    "                continue\n",
    "            split_path = dataset_paths[split]\n",
    "            new_dataset = ParallelDataset(src_file=split_path + SRC_SUFFIX,\n",
    "                                          tgt_file=split_path + TGT_SUFFIX,\n",
    "                                          src_vocab=self.src_vocab,\n",
    "                                          tgt_vocab=self.tgt_vocab)\n",
    "            if self.src_vocab is None:\n",
    "                self.src_vocab = new_dataset.src_vocab\n",
    "            if self.tgt_vocab is None:\n",
    "                self.tgt_vocab = new_dataset.tgt_vocab\n",
    "            self.datasets[split] = new_dataset\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_sequence(seq: torch.Tensor, max_length: int, padding_value: Union[int, float]) -> torch.Tensor:\n",
    "        seq_len = seq.shape[-1]\n",
    "        if seq_len < max_length:\n",
    "            return torch.cat(\n",
    "                [seq, torch.tensor([padding_value] * (max_length - seq_len), dtype=seq.dtype, device=seq.device)],\n",
    "                dim=-1\n",
    "            )\n",
    "        else:\n",
    "            return seq\n",
    "\n",
    "    def collate_fn(self, instances: List[ParallelInstance]) -> ParallelBatch:\n",
    "        \"\"\"Collates a list of instances and composes a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        instances : List[ParallelInstance]\n",
    "            A list of `ParallelInstance` to comprise.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch : ParallelBatch\n",
    "            A single `ParallelBatch` where tensors are batched instances.\n",
    "        \"\"\"\n",
    "        max_src_seq_len = max([x.src_seq.shape[0] for x in instances])\n",
    "        has_tgt = instances[0].tgt_seq is not None\n",
    "        max_tgt_seq_len = max([x.tgt_seq.shape[0] for x in instances]) if has_tgt else -1\n",
    "        return ParallelBatch(\n",
    "            src_seqs=torch.stack(\n",
    "                [\n",
    "                    self._pad_sequence(x.src_seq, max_length=max_src_seq_len,\n",
    "                                       padding_value=self.src_vocab.token2idx(str(self.src_vocab.pad().value)))\n",
    "                    for x in instances\n",
    "                ],\n",
    "                dim=0\n",
    "            ),\n",
    "            src_attention_mask=torch.stack(\n",
    "                [\n",
    "                    self._pad_sequence(\n",
    "                        seq=torch.ones_like(x.src_seq, dtype=torch.bool),\n",
    "                        max_length=max_src_seq_len,\n",
    "                        padding_value=0\n",
    "                    )\n",
    "                    for x in instances\n",
    "                ],\n",
    "                dim=0\n",
    "            ),\n",
    "            tgt_seqs=torch.stack(\n",
    "                [\n",
    "                    self._pad_sequence(x.tgt_seq, max_length=max_tgt_seq_len,\n",
    "                                       padding_value=self.tgt_vocab.token2idx(str(self.tgt_vocab.pad().value)))\n",
    "                    for x in instances\n",
    "                ],\n",
    "                dim=0\n",
    "            ) if has_tgt else None,\n",
    "            tgt_attention_mask=torch.stack(\n",
    "                [\n",
    "                    self._pad_sequence(\n",
    "                        seq=torch.ones_like(x.tgt_seq, dtype=torch.bool),\n",
    "                        max_length=max_tgt_seq_len,\n",
    "                        padding_value=0\n",
    "                    )\n",
    "                    for x in instances\n",
    "                ],\n",
    "                dim=0\n",
    "            ) if has_tgt else None,\n",
    "            src_tokens=[x.src_tokens for x in instances],\n",
    "            tgt_tokens=[x.tgt_tokens for x in instances] if has_tgt else None\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.datasets['train'],\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=self.shuffle,\n",
    "                          collate_fn=lambda x: self.collate_fn(x))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.datasets['val'],\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: self.collate_fn(x))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.datasets['test'],\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: self.collate_fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0ELchjhsQGg8"
   },
   "outputs": [],
   "source": [
    "cmudict_corpus = ParallelDataModule(\n",
    "    dataset_paths={'train': CMUDICT_TRAIN, 'val': CMUDICT_DEV},\n",
    "    src_vocab=cmudict_src_vocab,\n",
    "    tgt_vocab=cmudict_tgt_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed9y_j2KQGg8",
    "outputId": "e3efab14-e54d-4408-8c3a-f69db8c33a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelBatch(src_seqs=tensor([[18,  9, 26, 10, 10, 22,  7, 30, 11, 23,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 28, 23, 14, 26, 11, 21, 12, 22, 26,  7, 22, 13, 22,  7, 30,  6],\n",
      "        [18, 21,  7, 19, 21,  5, 11, 22,  2, 25, 23,  4,  6, 15, 15, 15, 15],\n",
      "        [18, 32,  1,  7,  4,  1, 25,  1, 28,  6, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 30, 28,  1,  7,  4, 22, 11, 11, 26,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 19, 28, 26, 12, 26, 12, 20, 19, 22,  9,  1, 11,  6, 15, 15, 15],\n",
      "        [18,  5, 28, 23,  1, 12, 25, 12,  1, 32, 22,  7, 30,  6, 15, 15, 15],\n",
      "        [18,  5, 23,  7, 23,  4, 22,  9, 12,  1,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 21,  7, 26, 29, 29, 22,  9, 22,  1, 11,  6, 15, 15, 15, 15, 15],\n",
      "        [18,  2, 12, 21,  7,  7, 22,  7, 30, 11, 20,  6, 15, 15, 15, 15, 15],\n",
      "        [18,  4, 22,  2,  9, 21,  2,  2, 23,  2,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 28, 23,  9, 26,  7, 29, 22, 30, 21, 28, 23,  4,  6, 15, 15, 15],\n",
      "        [18, 29, 28, 21,  2, 12, 28,  1, 12, 23,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 11, 22, 30,  1, 10, 23,  7, 12,  2,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 12, 28,  1,  7,  2,  2, 23, 31, 21,  1, 11,  2,  6, 15, 15, 15],\n",
      "        [18, 25, 21, 10,  1,  7, 22, 12, 20,  3,  2,  6, 15, 15, 15, 15, 15],\n",
      "        [18,  9, 26, 11, 26,  7, 22,  1, 11, 22,  2, 12,  2,  6, 15, 15, 15],\n",
      "        [18, 21,  7,  2,  1, 12, 22,  2, 29,  1,  9, 12, 26, 28, 20,  6, 15],\n",
      "        [18, 26, 19, 25, 12, 25,  1, 11, 10, 26, 11, 26, 30, 20,  6, 15, 15],\n",
      "        [18, 10, 26,  7, 12, 29, 26, 28,  4,  6, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [18,  5, 22, 26, 30, 28,  1, 19, 25, 23, 28,  6, 15, 15, 15, 15, 15],\n",
      "        [18, 23, 11, 23,  9, 12, 26, 28,  1, 11,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 12,  1, 32, 23,  2, 25, 22, 12,  1,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 10,  1, 11, 29, 26, 28, 10,  1, 12, 22, 26,  7,  6, 15, 15, 15],\n",
      "        [18, 19,  2, 20,  9, 25, 26,  1,  7,  1, 11, 20,  2, 22,  2,  6, 15],\n",
      "        [18, 19, 23,  7,  1, 11, 22, 13, 23,  2,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18,  2,  1,  7,  8,  2,  1, 11, 14,  1,  4, 26, 28,  6, 15, 15, 15],\n",
      "        [18,  2, 26,  9, 22, 26, 23,  9, 26,  7, 26, 10, 22,  9,  6, 15, 15],\n",
      "        [18, 23, 28, 20, 12, 25, 28, 26, 19, 26, 22, 23, 12, 22,  7,  6, 15],\n",
      "        [18, 10,  1,  9, 25, 22,  7,  1, 12, 22, 26,  7,  6, 15, 15, 15, 15],\n",
      "        [18, 30, 11, 26, 30, 26,  0,  2, 32, 22,  6, 15, 15, 15, 15, 15, 15],\n",
      "        [18, 19, 28, 26, 10, 22,  7, 23,  7, 12, 11, 20,  6, 15, 15, 15, 15]]), src_attention_mask=tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False]]), src_tokens=[['c', 'o', 'm', 'm', 'i', 'n', 'g', 'l', 'e'], ['r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'i', 'n', 'g'], ['u', 'n', 'p', 'u', 'b', 'l', 'i', 's', 'h', 'e', 'd'], ['k', 'a', 'n', 'd', 'a', 'h', 'a', 'r'], ['g', 'r', 'a', 'n', 'd', 'i', 'l', 'l', 'o'], ['p', 'r', 'o', 't', 'o', 't', 'y', 'p', 'i', 'c', 'a', 'l'], ['b', 'r', 'e', 'a', 't', 'h', 't', 'a', 'k', 'i', 'n', 'g'], ['b', 'e', 'n', 'e', 'd', 'i', 'c', 't', 'a'], ['u', 'n', 'o', 'f', 'f', 'i', 'c', 'i', 'a', 'l'], ['s', 't', 'u', 'n', 'n', 'i', 'n', 'g', 'l', 'y'], ['d', 'i', 's', 'c', 'u', 's', 's', 'e', 's'], ['r', 'e', 'c', 'o', 'n', 'f', 'i', 'g', 'u', 'r', 'e', 'd'], ['f', 'r', 'u', 's', 't', 'r', 'a', 't', 'e'], ['l', 'i', 'g', 'a', 'm', 'e', 'n', 't', 's'], ['t', 'r', 'a', 'n', 's', 's', 'e', 'x', 'u', 'a', 'l', 's'], ['h', 'u', 'm', 'a', 'n', 'i', 't', 'y', \"'\", 's'], ['c', 'o', 'l', 'o', 'n', 'i', 'a', 'l', 'i', 's', 't', 's'], ['u', 'n', 's', 'a', 't', 'i', 's', 'f', 'a', 'c', 't', 'o', 'r', 'y'], ['o', 'p', 'h', 't', 'h', 'a', 'l', 'm', 'o', 'l', 'o', 'g', 'y'], ['m', 'o', 'n', 't', 'f', 'o', 'r', 'd'], ['b', 'i', 'o', 'g', 'r', 'a', 'p', 'h', 'e', 'r'], ['e', 'l', 'e', 'c', 't', 'o', 'r', 'a', 'l'], ['t', 'a', 'k', 'e', 's', 'h', 'i', 't', 'a'], ['m', 'a', 'l', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n'], ['p', 's', 'y', 'c', 'h', 'o', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's'], ['p', 'e', 'n', 'a', 'l', 'i', 'z', 'e', 's'], ['s', 'a', 'n', '-', 's', 'a', 'l', 'v', 'a', 'd', 'o', 'r'], ['s', 'o', 'c', 'i', 'o', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c'], ['e', 'r', 'y', 't', 'h', 'r', 'o', 'p', 'o', 'i', 'e', 't', 'i', 'n'], ['m', 'a', 'c', 'h', 'i', 'n', 'a', 't', 'i', 'o', 'n'], ['g', 'l', 'o', 'g', 'o', 'w', 's', 'k', 'i'], ['p', 'r', 'o', 'm', 'i', 'n', 'e', 'n', 't', 'l', 'y']], tgt_seqs=tensor([[ 7, 73, 52, 65, 46, 75, 24, 52, 17, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7,  2, 41,  5, 52, 17, 38, 36, 52, 31, 16, 28,  1, 75, 23],\n",
      "        [ 7, 52, 31, 29, 48, 11, 17,  1, 36, 68, 23, 67, 67, 67, 67],\n",
      "        [ 7, 73,  4, 31, 53, 52, 49, 45,  2, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 24,  2, 22, 31, 53, 46, 17, 20, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 29,  2, 15, 68, 52, 68, 46, 29,  1, 73, 52, 17, 23, 67],\n",
      "        [ 7, 11,  2, 64, 39, 68, 74, 73,  1, 75, 23, 67, 67, 67, 67],\n",
      "        [ 7, 11, 64, 31, 52, 53, 61, 73, 68, 52, 23, 67, 67, 67, 67],\n",
      "        [ 7, 32, 31, 52, 62, 46, 36, 52, 17, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 33, 68, 48, 31,  1, 75, 17, 61, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 53,  1, 33, 73, 48, 33, 52, 28, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7,  2, 25, 73, 52, 31, 62, 46, 24,  0, 71, 53, 23, 67, 67],\n",
      "        [ 7, 62,  2, 48, 33, 68,  2, 74, 68, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 17, 46, 24, 52, 65, 52, 31, 68, 33, 23, 67, 67, 67, 67],\n",
      "        [ 7, 68,  2, 22, 31, 33, 64, 73, 36,  0, 66, 52, 17, 28, 23],\n",
      "        [ 7, 49,  0, 66, 65,  4, 31,  1, 68, 61, 28, 23, 67, 67, 67],\n",
      "        [ 7, 73, 52, 17, 50, 31, 61, 52, 17,  1, 33, 68, 33, 23, 67],\n",
      "        [ 7, 32, 31, 33, 52, 68,  1, 33, 62,  4, 73, 68, 71, 61, 23],\n",
      "        [ 7, 60, 29, 39, 52, 65,  6, 17, 52, 26, 61, 23, 67, 67, 67],\n",
      "        [ 7, 65, 52, 31, 68, 62, 57,  2, 53, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 11, 63,  6, 24,  2, 52, 62, 71, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7,  1, 17, 64, 73, 68, 71, 52, 17, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 68, 60, 73, 52, 36, 51, 68, 60, 23, 67, 67, 67, 67, 67],\n",
      "        [ 7, 65, 44, 17, 62, 37,  2, 65,  8, 36, 52, 31, 23, 67, 67],\n",
      "        [ 7, 33, 16, 73, 20, 52, 31,  4, 17, 52, 33, 52, 33, 23, 67],\n",
      "        [ 7, 29, 64, 31, 52, 17, 16, 28,  1, 28, 23, 67, 67, 67, 67],\n",
      "        [ 7, 33,  4, 31, 33,  4, 17,  5, 52, 53, 19,  2, 23, 67, 67],\n",
      "        [ 7, 33, 20, 33, 25, 15, 41, 73, 52, 31,  6, 65,  1, 73, 23],\n",
      "        [ 7,  9,  2, 54, 39,  2, 52, 29, 70, 68,  1, 31, 23, 67, 67],\n",
      "        [ 7, 65, 44, 73, 52, 31,  8, 36, 52, 31, 23, 67, 67, 67, 67],\n",
      "        [ 7, 24, 17, 52, 24, 57, 62, 33, 73, 61, 23, 67, 67, 67, 67],\n",
      "        [ 7, 29,  2,  6, 65, 52, 31, 52, 31, 68, 17, 61, 23, 67, 67]]), tgt_attention_mask=tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False]]), tgt_tokens=[['K', 'AH0', 'M', 'IH1', 'NG', 'G', 'AH0', 'L'], ['R', 'EH2', 'V', 'AH0', 'L', 'UW1', 'SH', 'AH0', 'N', 'AY2', 'Z', 'IH0', 'NG'], ['AH0', 'N', 'P', 'AH1', 'B', 'L', 'IH0', 'SH', 'T'], ['K', 'AE1', 'N', 'D', 'AH0', 'HH', 'AA0', 'R'], ['G', 'R', 'AE0', 'N', 'D', 'IH1', 'L', 'OW0'], ['P', 'R', 'OW2', 'T', 'AH0', 'T', 'IH1', 'P', 'IH0', 'K', 'AH0', 'L'], ['B', 'R', 'EH1', 'TH', 'T', 'EY2', 'K', 'IH0', 'NG'], ['B', 'EH1', 'N', 'AH0', 'D', 'IY0', 'K', 'T', 'AH0'], ['AH2', 'N', 'AH0', 'F', 'IH1', 'SH', 'AH0', 'L'], ['S', 'T', 'AH1', 'N', 'IH0', 'NG', 'L', 'IY0'], ['D', 'IH0', 'S', 'K', 'AH1', 'S', 'AH0', 'Z'], ['R', 'IY2', 'K', 'AH0', 'N', 'F', 'IH1', 'G', 'Y', 'ER0', 'D'], ['F', 'R', 'AH1', 'S', 'T', 'R', 'EY2', 'T'], ['L', 'IH1', 'G', 'AH0', 'M', 'AH0', 'N', 'T', 'S'], ['T', 'R', 'AE0', 'N', 'S', 'EH1', 'K', 'SH', 'Y', 'UW0', 'AH0', 'L', 'Z'], ['HH', 'Y', 'UW0', 'M', 'AE1', 'N', 'IH0', 'T', 'IY0', 'Z'], ['K', 'AH0', 'L', 'OW1', 'N', 'IY0', 'AH0', 'L', 'IH0', 'S', 'T', 'S'], ['AH2', 'N', 'S', 'AH0', 'T', 'IH0', 'S', 'F', 'AE1', 'K', 'T', 'ER0', 'IY0'], ['AA2', 'P', 'TH', 'AH0', 'M', 'AA1', 'L', 'AH0', 'JH', 'IY0'], ['M', 'AH0', 'N', 'T', 'F', 'AO1', 'R', 'D'], ['B', 'AY0', 'AA1', 'G', 'R', 'AH0', 'F', 'ER0'], ['IH0', 'L', 'EH1', 'K', 'T', 'ER0', 'AH0', 'L'], ['T', 'AA2', 'K', 'AH0', 'SH', 'IY1', 'T', 'AA2'], ['M', 'AE2', 'L', 'F', 'AO0', 'R', 'M', 'EY1', 'SH', 'AH0', 'N'], ['S', 'AY2', 'K', 'OW0', 'AH0', 'N', 'AE1', 'L', 'AH0', 'S', 'AH0', 'S'], ['P', 'EH1', 'N', 'AH0', 'L', 'AY2', 'Z', 'IH0', 'Z'], ['S', 'AE1', 'N', 'S', 'AE1', 'L', 'V', 'AH0', 'D', 'AO2', 'R'], ['S', 'OW0', 'S', 'IY2', 'OW2', 'EH2', 'K', 'AH0', 'N', 'AA1', 'M', 'IH0', 'K'], ['EH0', 'R', 'IH2', 'TH', 'R', 'AH0', 'P', 'OY1', 'T', 'IH0', 'N'], ['M', 'AE2', 'K', 'AH0', 'N', 'EY1', 'SH', 'AH0', 'N'], ['G', 'L', 'AH0', 'G', 'AO1', 'F', 'S', 'K', 'IY0'], ['P', 'R', 'AA1', 'M', 'AH0', 'N', 'AH0', 'N', 'T', 'L', 'IY0']])\n"
     ]
    }
   ],
   "source": [
    "# Please take a look at the output to get the sense of what the data looks like\n",
    "for i, x in enumerate(cmudict_corpus.train_dataloader()):\n",
    "    if i > 0:\n",
    "        break\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2LElUe4QGg8"
   },
   "source": [
    "## Evaluation Routine\n",
    "\n",
    "The evaluation routine is to take model predictions and compare them with gold labels. The measurements for the quality of predictions is called metric. The routine also includes the mechanism to integrate such metrics to the training loop so that model selection techniques such as [early stopping](https://en.wikipedia.org/wiki/Early_stopping) can be employed.\n",
    "\n",
    "In this section, we will walk you through how to implement a metric, say Character Error Rate, and how to integrate it into the existing Lightning training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1EUZL3iQGg8"
   },
   "source": [
    "### Metric: Character Error Rate (CER)\n",
    "\n",
    "We are going to evaluate our model's predictions using Character Error Rate (CER). This measures the number of edits (insertions, deletions and substitutions) needed to convert our model's prediction to the correct output sequence. [Edit distance computation](https://nlp.stanford.edu/IR-book/html/htmledition/edit-distance-1.html) is one of the popular applications of dynamic programming, and is used for measuring character/phone/word error rates in speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUvnSHUiQGg9"
   },
   "source": [
    "Complete the following function which takes two sequences (list of characters) and computes the edit distance between them. Another function computes statistics (errors and totals) that will later be used to get CER. These are what we called functional primitives, which themselves can be used to compute edit distance and CER without getting involved in any Lightning contexts.  _(12 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xyot20NOQGg9"
   },
   "outputs": [],
   "source": [
    "def compute_edit_distance(prediction_tokens: List[str], reference_tokens: List[str]) -> int:\n",
    "    \"\"\"Computes edit distance for two sequences using dynamic programming.\n",
    "    This is actually a LeetCode problem :-)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_tokens : List[str]\n",
    "        A tokenized predicted sentence.\n",
    "    reference_tokens : List[str]\n",
    "        A tokenized reference sentence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance : int\n",
    "        Edit distance between the predicted sentence and the reference sentence.\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    s1 = len(prediction_tokens)\n",
    "    s2 = len(reference_tokens)\n",
    "    if s1==0:\n",
    "      return s2\n",
    "    if s2==0:\n",
    "      return s1\n",
    "    \n",
    "    # init m\n",
    "    m=[]\n",
    "    for i in range(s1+1):\n",
    "      m.append([0 for j in range(s2+1)])\n",
    "    \n",
    "    m[0][0] = 0\n",
    "    for i in range(1, s1+1):\n",
    "      m[i][0] = i\n",
    "    for j in range(1, s2+1):\n",
    "      m[0][j] = j\n",
    "\n",
    "    for i in range(1, s1+1):\n",
    "      for j in range(1, s2+1):\n",
    "        diagnol = m[i-1][j-1]\n",
    "        left = m[i-1][j] + 1\n",
    "        upper = m[i][j-1] + 1\n",
    "        if prediction_tokens[i-1]!=reference_tokens[j-1]:\n",
    "          diagnol+=1\n",
    "        m[i][j] = min(diagnol, left, upper)\n",
    "\n",
    "    distance = m[s1][s2]\n",
    "    return distance\n",
    "\n",
    "\n",
    "def update_cer(\n",
    "        preds: Union[str, List[str]],\n",
    "        targets: Union[str, List[str]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Updates the CER score with the current set of references and predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        preds: Transcription(s) to score as a string or list of strings\n",
    "        targets: Reference(s) for each speech input as a string or list of strings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Number of edit operations to get from the reference to the prediction, summed over all samples\n",
    "        Number of character overall references\n",
    "    \"\"\"\n",
    "    if isinstance(preds, str):\n",
    "        preds = [preds]\n",
    "    if isinstance(targets, str):\n",
    "        targets = [targets]\n",
    "    errors = torch.tensor(0, dtype=torch.float)\n",
    "    total = torch.tensor(0, dtype=torch.float)\n",
    "\n",
    "    # TODO: you have to compute edit distance for each pair of prediction and target\n",
    "    # Then update total errors and total number of references.\n",
    "    for pred, target in zip(preds, targets):\n",
    "      edit_dis = compute_edit_distance(pred, target)\n",
    "      errors += edit_dis\n",
    "      total += len(target)\n",
    "\n",
    "    return errors, total\n",
    "\n",
    "\n",
    "def compute_cer(errors: torch.Tensor, total: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the CER.\"\"\"\n",
    "    return errors/total\n",
    "\n",
    "\n",
    "def character_error_rate(preds: Union[str, List[str]], targets: Union[str, List[str]]) -> torch.Tensor:\n",
    "    \"\"\"character error rate is a common metric of the performance of an automatic speech recognition system. This\n",
    "    value indicates the percentage of characters that were incorrectly predicted. The lower the value, the better\n",
    "    the performance of the ASR system with a CER of 0 being a perfect score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        preds: Transcription(s) to score as a string or list of strings\n",
    "        targets: Reference(s) for each speech input as a string or list of strings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Character error rate score\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "        preds = [\"this is the prediction\", \"there is an other sample\"]\n",
    "        target = [\"this is the reference\", \"there is another one\"]\n",
    "        character_error_rate(preds=preds, targets=targets)\n",
    "        tensor(0.3415)\n",
    "    \"\"\"\n",
    "    errors, total = update_cer(preds, targets)\n",
    "    return compute_cer(errors, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "LB7oR5oSQGg9",
    "outputId": "57887353-5d44-4e46-bd61-52457106398c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>metric-cer-impl</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "metric-cer-impl results: All test cases passed!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"metric-cer-impl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8_A7vkdQGg9"
   },
   "source": [
    "With the above functional primitives implemented, we will then wrap them to be a `TorchMetric`, which can be integrated into Lightning.  _(5 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4Ry_ntLJQGg-"
   },
   "outputs": [],
   "source": [
    "class CharErrorRate(torchmetrics.Metric):\n",
    "    \"\"\" Character Error Rate metric wrapper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Character error rate score\n",
    "    \"\"\"\n",
    "    is_differentiable: bool = False\n",
    "    higher_is_better: bool = False\n",
    "    full_state_update: bool = False\n",
    "\n",
    "    error: torch.Tensor\n",
    "    total: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add_state(\"errors\", torch.tensor(0, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", torch.tensor(0, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Union[str, List[str]], targets: Union[str, List[str]]) -> None:\n",
    "        \"\"\"Stores references/predictions for computing Character Error Rate scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            preds : Union[str, List[str]]\n",
    "                Transcription(s) to score as a string or list of strings.\n",
    "            targets: Union[str, List[str]]\n",
    "                Reference(s) for each speech input as a string or list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        e, t = update_cer(preds, targets)\n",
    "        if self.errors == None:\n",
    "          self.errors = e\n",
    "        else:\n",
    "          self.errors += e\n",
    "        if self.total == None:\n",
    "          self.total = t\n",
    "        else:\n",
    "          self.total += t\n",
    "        \n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Calculates the character error rate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "           Character error rate score\n",
    "        \"\"\"\n",
    "        # if self.total != 0:\n",
    "        #   return self.errors.float() / self.total\n",
    "        # else:\n",
    "        #   return 0\n",
    "        return compute_cer(self.errors, self.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "yQ_IR6QoQGg-",
    "outputId": "26f25a8f-dbfa-4c35-d235-cf075ab4e8d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>metric-torchmetric-impl</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "metric-torchmetric-impl results: All test cases passed!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"metric-torchmetric-impl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94bdPi1Bkrkb"
   },
   "source": [
    "## Sequence-to-sequence model with encoder-decoder architecture\n",
    "\n",
    "In this homework, we require you to implement a sequence-to-sequence model that uses encoder-decoder architecture. You are free to implement the model with either RNN or Transformer. However, you have to bear in mind that the computing resource provided by Colab is limited, and the autograder is configured to run with CPU-only for up to 40 mins with 6GB memory. The autograder runtime is for inference only, so if your training takes relatively longer, it might still be fine with the autograder. To emphasize, you have to test your submission with the autograder as in previous homework.\n",
    "\n",
    "For people that are not familiar with the concept of encoder-decoder model, we refer you to read the [Chapter 10 in Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/10.pdf). You have at least understand what is an encoder-decoder architecture, what is sequence-to-sequence model, how the model is going to be trained, and how decoding works. Some important connections among the described task in the book chapter and this homework, and previous homework:\n",
    "- Machine Translation task is to translate from one language to the other. In this homework, you can imagine that the task is translating from English to phonemes. One obvious difference is that the target vocabulary is no longer another language, but phonemes.\n",
    "- In the previous homework, we played with language model, which is very similar to a decoder. For each step, we feed the decoder with what has been generated and ask it to predict a next token. This is the behavior happened at the inference time (using the model to do something). However, we also noticed that the behavior for training is different as we offset the sequence by 1. This is actually exactly the same as the inference, in which given a previous one you have to predict the next one; the only difference is that it happens in parallel as we assume a gold previous one is fed to the model. This is what we called [teacher forcing](https://cedar.buffalo.edu/~srihari/CSE676/10.2.1%20TeacherForcing.pdf). Please make sure you have understood these concepts before proceeding.\n",
    "- With the trained model, you have again to use decoding methods learned from the previous homework to decode output sequences.\n",
    "\n",
    "Note:\n",
    "- As this is teamwork, we expect people to figure out concepts collaboratively. You are free to ask questions on Piazza, and it would be better for students to answer others questions instead of fully relying on course staff. This would benefit each other in the sense of figuring unclear parts about these techniques.\n",
    "- The homework is open to different architecture designs as long as your model takes in one English sentence and spits one phoneme sequence.\n",
    "\n",
    "Grading:\n",
    "- There is a baseline (will be shown on gradescope) using the similar architecture as in the previous homework. Any submission reaches a comparable test score would be given __30 points__.\n",
    "- We set a cutoff above the baseline (will be announced later). Any submission reaches the cutoff would be given another __10 points__. This problem is considered to have __40 points__ in total.\n",
    "- There will also be a leaderboard. The top 15% performers would be given __15 points__, and top 30% performers would be given __7 points__. **These are considered as extra credits.**\n",
    "- As the homework is pretty open, we encourage students to discuss and answer questions for each other. **For individuals answering at least 5 questions, we would give extra credits ranging from __5 to 10 points__.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5EIGzkOgTNpO"
   },
   "outputs": [],
   "source": [
    "# This cell provides an example skeleton that you can implement encoder-decoder model\n",
    "# You are free to change the skeleton as long as your final model implements a `generate()` method (its signature is commented out).generation_utils.py` to understand how their interfaces work).\n",
    "# To use the provided skeleton, it would provide you with access to Huggingface transformers' generation API, which means that you can directly use GreedyDecoding and SamplingDecoding. However, we did not make the effort to make the implementation compatible with BeamDecoding. If you are interested, you could implement yourself (which requires you to dig into the hugging face `\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"Shifts input ids one token to the right.\"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    # print('shifted shape =', shifted_input_ids.shape)\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "class Encoder(PreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_size: int,\n",
    "                 hidden_size: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: float,\n",
    "                 pad_token_id: int,\n",
    "                 bos_token_id: int,\n",
    "                 eos_token_id: int):\n",
    "        super(Encoder, self).__init__(config=PretrainedConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id\n",
    "        ))\n",
    "\n",
    "        # Initialization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Configures the embedding - source side\n",
    "        # We name the embedding as `self.embed_tokens`\n",
    "        # which is aligned with the following get and set embeddings methods.\n",
    "        self.embed_tokens = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # Configures the network\n",
    "        # We assume using RNN here - this aligns with the output\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        # self.lstm = torch.nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # output\n",
    "        self.output = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.embed_tokens\n",
    "\n",
    "    # def set_input_embeddings(self, value: nn.Module):\n",
    "    #     return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                **kwargs) -> BaseModelOutput:\n",
    "        \"\"\"Encodes `input_ids` using some encoding networks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor\n",
    "            `src_seqs` in the shape of (batch_size, src_seq_len).\n",
    "        attention_mask : torch.Tensor\n",
    "            `1` means valid tokens, and `0` means invalid tokens.\n",
    "            `attention_mask.cpu().sum(-1)` can result in corresponding sequence length for\n",
    "            inputs.\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        `BaseModelOutput` that describes the encoder output.\n",
    "        \"\"\"\n",
    "        # Shape: (batch_size, sequence_length, embedding_size)\n",
    "        # Embeds the `input_ids` to embeddings\n",
    "\n",
    "        embedding = self.embed_tokens(input_ids)\n",
    "\n",
    "        # It would output a tuple consists:\n",
    "        #   `outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        #   `updated_hidden` shape: (num_layers, batch_size, hidden_size)\n",
    "        # Feed input embeddings to the RNN\n",
    "        # hidden = torch.zeros(self.num_layers, input_ids.shape[0], self.hidden_size)\n",
    "        \n",
    "        # updated_hidden, outputs = self.lstm(embedding, hidden)\n",
    "        outputs, updated_hidden = self.gru(embedding)\n",
    "        # outputs = self.combine_bidir(outputs, self.vocab_size)\n",
    "        # updated_hidden = self.combine_bidir(updated_hidden, self.vocab_size)\n",
    "        # Unpacks (back to padded)\n",
    "        # Unpack the `outputs` to:\n",
    "        #   `unpacked_outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        #   `output_lengths` shape: (batch_size)\n",
    "        # Using `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "        # unpacked_outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "        # You are free to change the outputs\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=outputs,\n",
    "            hidden_states=(updated_hidden,),\n",
    "            attentions=None  # Change this if you implemented attentions\n",
    "        )\n",
    "\n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.num_layers, 2, bsz-1, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "\n",
    "class Decoder(PreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_size: int,\n",
    "                 hidden_size: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: float,\n",
    "                 pad_token_id: int,\n",
    "                 bos_token_id: int,\n",
    "                 eos_token_id: int):\n",
    "        super(Decoder, self).__init__(config=PretrainedConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            is_decoder=True\n",
    "        ))\n",
    "\n",
    "\n",
    "        # Initialization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "\n",
    "        # Similar configurations as in the encoder\n",
    "        # But this time they are configured to use the target side vocab\n",
    "        self.embed_tokens = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # Configures the network\n",
    "        # We assume using RNN here - this aligns with the output\n",
    "        # self.lstm = torch.nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.multihead_attn = torch.nn.MultiheadAttention(embedding_size, 2, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=self.num_layers, bidirectional=False, batch_first=True)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        # return self.embed_tokens\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor = None,\n",
    "                encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "                encoder_last_hidden_state: Optional[torch.FloatTensor] = None,\n",
    "                decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "                **kwargs) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "        \"\"\"Decodes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor, optional\n",
    "            Input sequence ids on the decoder-side. This is different from what is\n",
    "            fed on the encoder-side. Please make sure you understand why they are\n",
    "            different from encoder-decoder slides.\n",
    "        encoder_hidden_states : torch.Tensor, optional\n",
    "            Hidden states from the encoder. This enables the model to proceed with\n",
    "            the information from the encoder.\n",
    "        decoder_attention_mask : torch.Tensor, optional\n",
    "            This is similar to the encoder attention mask.\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        `BaseModelOutputWithPastAndCrossAttentions` similar to the encoder outputs,\n",
    "        but this also leaves the space for implementing attention.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the decoder mask is not provided, we create one by using non-pad tokens.\n",
    "        if decoder_attention_mask is None:\n",
    "            decoder_attention_mask = input_ids != self.config.pad_token_id\n",
    "\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        # This is to give a hint on how encoder hidden states are passed to the module\n",
    "        # You are free to change it for your implementations.\n",
    "        hidden = encoder_hidden_states\n",
    "        # Shape: (batch_size, sequence_length, embedding_size)\n",
    "        # Similar thing for embeddings\n",
    "        embedding = self.embed_tokens(input_ids) #(sequence_length, batch_size)\n",
    "        batch_size, seq_len , embed_dim = embedding.shape\n",
    "\n",
    "        # attention layer\n",
    "        attn_output, attn_output_weights = self.multihead_attn(embedding, encoder_last_hidden_state , encoder_last_hidden_state)\n",
    "        combined_output = self.out(torch.cat((embedding, attn_output), dim=-1))\n",
    "\n",
    "        # It would output a tuple consists:\n",
    "        #   `outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        #   `updated_hidden` shape: (num_layers, batch_size, hidden_size)\n",
    "        # Similar to the encoder, but we can now use the hidden states from the encoder instead of `None`\n",
    "        # updated_hidden, outputs = self.lstm(embedding, hidden)\n",
    "        # print('input here =', embedding.shape)\n",
    "        # print('hidden size =', hidden.shape)\n",
    "        # print('output size =', output.shape)\n",
    "        outputs, updated_hidden = self.gru(combined_output, hidden)\n",
    "        # print('output here =', outputs.shape)\n",
    "        outputs = self.log_softmax(outputs)\n",
    "\n",
    "        # Unpacks (back to padded)\n",
    "        # Unpack the `outputs` to:\n",
    "        #   `unpacked_outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        #   `output_lengths` shape: (batch_size)\n",
    "        # Using `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "        # unpacked_outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "\n",
    "        # You are free to change the outputs\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=outputs,\n",
    "            hidden_states=(outputs,),\n",
    "        )\n",
    "\n",
    "\n",
    "class EncoderDecoder(PreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 src_vocab: Vocabulary,\n",
    "                 tgt_vocab: Vocabulary,\n",
    "                 embedding_size: int,\n",
    "                 hidden_size: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: float, ):\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_vocab_size = len(src_vocab)\n",
    "        self.tgt_vocab_size = len(tgt_vocab)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # You are free to modify the `__init__` to accommodate your model design.\n",
    "        super(EncoderDecoder, self).__init__(config=PretrainedConfig(\n",
    "            decoder_start_token_id=self.tgt_vocab.bos_id(),\n",
    "            is_encoder_decoder=True  # Tells the generation API the model arch\n",
    "        ))\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=self.src_vocab_size,\n",
    "            embedding_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            pad_token_id=self.src_vocab.pad_id(),\n",
    "            bos_token_id=self.src_vocab.bos_id(),\n",
    "            eos_token_id=self.src_vocab.eos_id()\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=self.tgt_vocab_size,\n",
    "            embedding_size=embedding_size * 2,\n",
    "            hidden_size=hidden_size * 2,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            pad_token_id=self.tgt_vocab.pad_id(),\n",
    "            bos_token_id=self.tgt_vocab.bos_id(),\n",
    "            eos_token_id=self.tgt_vocab.eos_id()\n",
    "        )\n",
    "\n",
    "        # Classification head for generating tokens for the target side\n",
    "        self.lm_head = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=self.tgt_vocab.pad_id())\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        self.lm_head = value\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    # The `generate()` method is commented out\n",
    "    # It should take `input_ids`, which are batched `src_seqs`, and generate their\n",
    "    # corresponding outputs using some decoding algorithms; such algorithms can be\n",
    "    # what we learned from the previous homework.\n",
    "    # If you decided to base your submission on the current skeleton, then you should\n",
    "    # be able to directly use the huggingface generation APIs.\n",
    "    #\n",
    "    # def generate(self, input_ids, attention_mask, **kwargs) -> torch.Tensor:\n",
    "    #     # #\"input_ids\": None, \"encoder_outputs\": encoder_outputs, \"decoder_input_ids\": decoder_input_ids\n",
    "       \n",
    "    #     encoder = self.get_encoder() #last_hidden_state=outputs, hidden_states=(updated_hidden,), attentions=None\n",
    "    #     decoder = self.get_decoder() # last_hidden_state=outputs, hidden_states=(updated_hidden,), attentions=None\n",
    "      \n",
    "    #     # - At the beginning, the generation module calls to get encoder outputs, which will\n",
    "    #     # then be used across all decoding steps.\n",
    "    #     # - This method prepares input for each decoding step. After each step, the newly\n",
    "    #     # decoded token would be added to update `decoder_input_ids`.\n",
    "    #     seq2seqLMoutputs = self.forward(input_ids)\n",
    "    #     print('seq output =', seq2seqLMoutputs)\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "            decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "            encoder_outputs: Optional[BaseModelOutput] = None,\n",
    "            labels: Optional[torch.LongTensor] = None,\n",
    "            **kwargs\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "        \"\"\"Runs the encoder-decdoer model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.LongTensor, optional\n",
    "            `src_seqs` to the encoder.\n",
    "        attention_mask : torch.Tensor, optional\n",
    "            Encoder side mask for indicating valid tokens (non-pad).\n",
    "        decoder_input_ids : torch.LongTensor, optional\n",
    "            Decoder side input tokens.\n",
    "        decoder_attention_mask : torch.Tensor, optional\n",
    "            Decoder side mask for indivating valid tokens (non-pad).\n",
    "        encoder_outputs : BaseModelOutput, optional\n",
    "            The output from encoder. This is to avoid re-running the encoder\n",
    "            during the decoding as the encoded information will not be changed.\n",
    "        labels : torch.LongTensor, optional\n",
    "            This is used to indicate output labels. Recall that we did language\n",
    "            modelling in hw2, where the labels are input tokens shifted by 1.\n",
    "            In this homework, as we have output different from the input, we use\n",
    "            the target sequence as the label.\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        `Seq2SeqLMOutput` see corresponding definitions.\n",
    "        \"\"\"\n",
    "        # Automatically creates decoder_input_ids from\n",
    "        # input_ids if no decoder_input_ids are provided\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.decoder.config.pad_token_id, self.decoder.config.bos_token_id\n",
    "                )\n",
    "                # reverse_decode_input_ids = torch.flip(decoder_input_ids.clone(), dim=1)\n",
    "                # decoder_input_ids = torch.cat((decoder_input_ids, reverse_decode_input_ids))\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            # Run encoder\n",
    "            # encoder_outputs= self.encoder(input_ids)\n",
    "            encoder_outputs= self.encoder(input_ids)\n",
    "            \n",
    "        # RUN decoder\n",
    "        #   `outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        #   `updated_hidden` shape: (num_layers, batch_size, hidden_size)\n",
    "        encoder_hidden = encoder_outputs.hidden_states[0]\n",
    "        _, batch_size, hidden_size = encoder_hidden.shape\n",
    "        encoder_hidden = encoder_hidden.view(self.num_layers, 2, batch_size, -1).transpose(1, 2).contiguous()\n",
    "        encoder_hidden = encoder_hidden.view(self.num_layers, batch_size, -1)\n",
    "\n",
    "        #beambeam search\n",
    "        _, batch_size, double_hidden_size = encoder_hidden.shape\n",
    "        num_beams = decoder_input_ids.shape[0]//batch_size\n",
    "\n",
    "        #to be compatible with decoder input, we reshape it to (num_layer,batch_size*num_beams,hidden_size)\n",
    "        encoder_hidden = encoder_hidden.unsqueeze(2).expand(self.num_layers,batch_size,num_beams,double_hidden_size)\n",
    "        encoder_hidden = encoder_hidden.contiguous().view(self.num_layers,batch_size*num_beams,double_hidden_size)\n",
    "\n",
    "        #print(\"decoder_input_ids: \",decoder_input_ids)\n",
    "        decoder_outputs = self.decoder(decoder_input_ids, encoder_hidden, encoder_outputs.last_hidden_state )\n",
    "        \n",
    "        # Passes the decoder output to LM head to get a distribution\n",
    "        # over the target vocabulary\n",
    "        # This part is a hint for how to use the decoder output.\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "      \n",
    "        # This is needed for cross entropy loss\n",
    "        # Shape: (batch_size * sequence_length, num_classes)\n",
    "       \n",
    "        # print('output shape =', decoder_outputs.last_hidden_state.shape)\n",
    "        # print('logits shape =', lm_logits.shape )\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            # print('labels =', labels.shape)\n",
    "            # Compute the loss function - similar to hw2\n",
    "            flattened_output_sequence = labels.view(-1)\n",
    "            logits_flatten = lm_logits.view(flattened_output_sequence.shape[0], -1)\n",
    "            lm_loss = self.loss(logits_flatten, flattened_output_sequence)\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=lm_loss,\n",
    "            logits=lm_logits,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self,\n",
    "            decoder_input_ids: torch.Tensor,\n",
    "            # attention_mask: Optional[torch.Tensor]=None,\n",
    "            encoder_outputs: Optional[BaseModelOutput] = None,\n",
    "            decoder_outputs: Optional[BaseModelOutputWithPastAndCrossAttentions] = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"Prepares the inputs for the generation mixin from huggingface.\n",
    "        This is the key interface to make sure that you could use the generations\n",
    "        implemented by huggingface.\n",
    "\n",
    "        You can imagine that:\n",
    "        - At the beginning, the generation module calls to get encoder outputs, which will\n",
    "        then be used across all decoding steps.\n",
    "        - This method prepares input for each decoding step. After each step, the newly\n",
    "        decoded token would be added to update `decoder_input_ids`.\n",
    "        - As we implemented `decoder_attention_mask` generation in `Decoder`, there is no\n",
    "        need to process attention mask in this method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        decoder_input_ids : torch.Tensor\n",
    "            Input ids on the decoder side (not the encoder side)! Be careful with the vocab.\n",
    "        encoder_outputs : BaseModelOutput, optional\n",
    "            The encoded information once the encoding is done with the encoder. This is to\n",
    "            have the encoder outputs reused across the decoding steps.\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Inputs to the `Decoder`. This is literally what being fed into the decoder.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "             \"decider_outputs\": decoder_outputs, # I tried to add decoder outputs but it isn't assigned values\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8CzSCZaTQsJ",
    "outputId": "19d65b18-ea83-4ec3-8579-764d986503c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21],\n",
       "        [ 7, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please instantiate your model here.\n",
    "# This will be used to do the sanity check for test cases.\n",
    "\n",
    "test_model = EncoderDecoder(src_vocab=cmudict_corpus.src_vocab, tgt_vocab=cmudict_corpus.tgt_vocab,\n",
    "embedding_size=256, hidden_size=256, num_layers=4, dropout=0.6)\n",
    "\n",
    "test_model.generate(\n",
    "                    input_ids=torch.tensor([[6, 13, 12, 10], [6, 2, 10, 13]], dtype=torch.long),\n",
    "                    attention_mask=torch.tensor([[True, True, True, True], [True, True, True, False]], dtype=torch.bool)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAp12Txlq80b",
    "outputId": "781e0169-b079-4856-ae2f-69515ec990b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 14, 14, 14, 14, 14, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "         33, 33],\n",
       "        [ 7, 14, 14, 14, 14, 14, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "         33, 33]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#beam search check\n",
    "test_model = EncoderDecoder(src_vocab=cmudict_corpus.src_vocab, tgt_vocab=cmudict_corpus.tgt_vocab,\n",
    "embedding_size=6, hidden_size=6, num_layers=4, dropout=0.6)\n",
    "test_model.generate(\n",
    "                    input_ids=torch.tensor([[11, 13,3], [1, 2,5]], dtype=torch.long),\n",
    "                    attention_mask=torch.tensor([[True, True, True, True], [True, True, True, False]], dtype=torch.bool),\n",
    "                    num_beams=4\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "UVmDnIHXQGhB",
    "outputId": "27ba6afa-add1-452b-ff6e-69df380e7ef5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>model-generate-checks</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "model-generate-checks results: All test cases passed!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"model-generate-checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "H5xbwwLMQGhC"
   },
   "outputs": [],
   "source": [
    "def wrapped_generate(model_to_wrap: nn.Module, **kwargs) -> torch.Tensor:\n",
    "    \"\"\"Wraps the generate method. This function is what will be actually\n",
    "    called by the evaluation routine for the leaderboard.\n",
    "\n",
    "    In this function, you can wrap your `generate()` method to allow\n",
    "    different generation configurations to be used at the test time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_to_wrap : nn.Module\n",
    "        Your encoder-decoder model.\n",
    "    kwargs\n",
    "        Argument dict that passes all arguments to the generate function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Generated phoneme sequences in the form of torch.Tensor.\n",
    "    \"\"\"\n",
    "    # This is a compatible version with the above `EncoderDecoder` model.\n",
    "    # If you would like to enable SamplingDecoding here, you can make it\n",
    "    #       return model.generate(do_sample=True, **kwargs)\n",
    "    #return model_to_wrap.generate(num_beams=1, do_sample=False, **kwargs)\n",
    "    #beam search\n",
    "    return model_to_wrap.generate(num_beams=5, do_sample=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-5fdYlQZQGhC"
   },
   "outputs": [],
   "source": [
    "class PhonemeGenerationTask(pl.LightningModule):\n",
    "    \"\"\"Wraps a PyTorch module as a Lightning Module for the phoneme generation task.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 src_vocab: Vocabulary,\n",
    "                 tgt_vocab: Vocabulary,\n",
    "                 learning_rate: float = 0.001):\n",
    "        super(PhonemeGenerationTask, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "        self.cer = CharErrorRate()\n",
    "\n",
    "    def training_step(self, batch: ParallelBatch) -> torch.Tensor:\n",
    "        \"\"\"Defines the training step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : ParallelBatch\n",
    "            The batched training instances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            The loss computed from the seq-to-seq model.\n",
    "        \"\"\"\n",
    "        # Please make necessary modifications to accommodate your design\n",
    "        outputs = self.model(\n",
    "            input_ids=batch.src_seqs,\n",
    "            attention_mask=batch.src_attention_mask,\n",
    "            labels=batch.tgt_seqs\n",
    "        )\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch: ParallelBatch, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Defines the validation step - for this module, we have the same\n",
    "        training and validation behaviors. Usually, we would compute a metric that is\n",
    "        used to select the best performing model checkpoint.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : ParallelBatch\n",
    "            The batched training instances.\n",
    "        batch_idx: int\n",
    "            The index of the batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            The loss computed using CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        # You are free to modify this function to ensure they are being called correctly.\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=batch.src_seqs,\n",
    "            attention_mask=batch.src_attention_mask,\n",
    "        )\n",
    "        decoded_outputs = decode_as_str(self.tgt_vocab, outputs)\n",
    "        curr_cer = self.cer(decoded_outputs, [' '.join(s) for s in batch.tgt_tokens])\n",
    "        self.log('val_cer', self.cer)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configures optimizers for the training.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575,
     "referenced_widgets": [
      "de27f188d4114123bb66e66f195dcbef",
      "0e879ef2db95468a9b7351437ac3ceae",
      "b62316570fd94b5db329cb31087800a9",
      "39ef516b8e4f4418a83b7785e1e2d385",
      "00871fcbab584ce5ae4e29f0f8199fb1",
      "b78ce547b59e4058a5efb06ac54eba3f",
      "7408378e21a143d6b736aa02e34c7b87",
      "a6b95598f61e42d59583830766772da1",
      "4a239b33f9fb4988a38ea7e670ea237c",
      "847ef212432e405ba2b15abc24865efb",
      "e74e244e7fbd4385a40944f25d962c0e",
      "ba016f47051748048d3ea3eacb040cb5",
      "4862785842d64e3e9db5c78c68f3e02e",
      "45608cd6ddea40728671826eccac2673",
      "519ea72dc41943e9b00dfe21c3920017",
      "031439a5a65744e99e0fdb9655ad7be2",
      "ce99764cfb40433aa86881089d1fbb24",
      "2529a46a24be479291d2c1c1a533c502",
      "f02b8c4b7dfe4ab4bc21c58376d23d4b",
      "94f75cd5ed664c248b75812669ecec65",
      "dff2b21aa3cb41b8a2e1531893b587e4",
      "eed118af1fe2427f9995d8ba8d521b60",
      "a312dfb435d04eeab0ddd93800564bd5",
      "18b748d47ef0485bb1a9923794d7e00c",
      "748a44938cb242bb868aeebec8e16942",
      "88b1c6e20208418da874083d41c00252",
      "c1fd5ef1ddbf4ef6844e619eaeeb439e",
      "164b4a43a21b4921b2a1fe5a4edc2570",
      "0d57dd312130445c917c4c35d1865129",
      "d8e067092f5f4469bf24f99fd6778ed6",
      "d44ce71756f44604a4874c6c2b0b650e",
      "2472f4bff1b745c3858452c2cc79d922",
      "68f0e413694d4f8fa1c6a0be292d4cdd",
      "dd3cde3982274b1aade898f69be605e2",
      "1ae043b338d24d7b9c09797665f7b0e3",
      "fa48b2f0eac54c808ee3893b94f4b4ef",
      "236e2db55ee84df3be06e34fe00827c0",
      "7992cbbd9b52433589823b4cd848cf26",
      "7365ff10ad0e42eaac9b6568598c4a1c",
      "46340253660048faacf4e415ef6bace5",
      "8da7de35a3624256a75215a3b9d760c3",
      "fba1cce16ccf420487e03472b79f212a",
      "2879f3f5cd84410794b2b7966535d46d",
      "874c4214bddd4b4d82b0516bb2ea0383",
      "b721507d45fb42e1b7a34ad6feeb49e2",
      "cd47ffdf4ac94160ab9a708c08d0869e",
      "9dac030e781e4204a50b7c025acc29ca",
      "5e715cefaf2c4520971aad5ca8cc4baa",
      "8314c02114c042709583f55e64d8e3d1",
      "da8c72e907394b35ac5621c53d8f46f8",
      "7fd9a0405934444ba336e97be71b2afe",
      "651fb14b68a14d0b985d6edbf26f8d93",
      "0fe49f11dce94ce095bce682c7142e2b",
      "cd0b633e00f349aca38e19cfaa83f77b",
      "e5707b91989b4711bc8622b54e54d0c8",
      "0f34b9ecd14148b58e9dc776d6917b30",
      "5a19bfc16d434ddbbce44b68c3749764",
      "053d447d9d744c48b608a582859f8e02",
      "659bd89df80d43f4bb5b90d37ffe91ec",
      "3afbc886957949c0afe68c813c75f48f",
      "87937aa39aba43beb25518b72a696a12",
      "5b87e467d54246cb8d0810763ea16c40",
      "ac126008fb4c426780dcddca28537c8f",
      "71d92538baea4082aaa970d81ceddd19",
      "033c930004af46128539d3cb0694e33d",
      "e2dfef2f341d45329ee016afbc897f1e",
      "a21d5126f2bb444f92e984b7c95e04a1",
      "8e15fb69cf01416c80953a8700540485",
      "f18c03bbbf864437bc5aebb8278d86a8",
      "7a2cf8d64a044558aaab5c746d1ba536",
      "7ae5fb83aa0647ea9a75f5e7d39e3169",
      "50d2c1bf58bd4268bdaca0699b485afa",
      "71dddaa70b7d41f0a2253e3bb46e6867",
      "00a53e6e88644a38874457b7268e6927"
     ]
    },
    "id": "PnD15zxMQGhC",
    "outputId": "0f1c99b4-7f42-41a6-b17a-937fb9dea105"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | EncoderDecoder | 3.1 M \n",
      "1 | cer   | CharErrorRate  | 0     \n",
      "-----------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.434    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de27f188d4114123bb66e66f195dcbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba016f47051748048d3ea3eacb040cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a312dfb435d04eeab0ddd93800564bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3cde3982274b1aade898f69be605e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae043b338d24d7b9c09797665f7b0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48b2f0eac54c808ee3893b94f4b4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236e2db55ee84df3be06e34fe00827c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7992cbbd9b52433589823b4cd848cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7365ff10ad0e42eaac9b6568598c4a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46340253660048faacf4e415ef6bace5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da7de35a3624256a75215a3b9d760c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba1cce16ccf420487e03472b79f212a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe49f11dce94ce095bce682c7142e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d92538baea4082aaa970d81ceddd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=12` reached.\n"
     ]
    }
   ],
   "source": [
    "# Please modify the model and trainer configurations to accommodate your design\n",
    "encoder_decoder_model = EncoderDecoder(\n",
    "    src_vocab=cmudict_src_vocab,\n",
    "    tgt_vocab=cmudict_tgt_vocab,\n",
    "    embedding_size=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=4,\n",
    "    dropout=0.7,\n",
    ")\n",
    "if is_autograder:  # You have to make sure that you checkpoint can be correctly loaded by the autograder \n",
    "    CHECKPOINT_TO_LOAD = 'model.ckpt'  # Please replace this to your checkpoint file name\n",
    "    phoneme_gen_pl_module = PhonemeGenerationTask.load_from_checkpoint(\n",
    "        checkpoint_path=CHECKPOINT_TO_LOAD,\n",
    "        src_vocab=cmudict_corpus.src_vocab,\n",
    "        tgt_vocab=cmudict_corpus.tgt_vocab,\n",
    "        model=encoder_decoder_model\n",
    "    )\n",
    "else:\n",
    "    # In the student mode, a new model would be trained\n",
    "    # You are allowed to change training hyperparameters\n",
    "    # But you are not allowed to create a new task\n",
    "\n",
    "    phoneme_gen_pl_module = PhonemeGenerationTask(\n",
    "        model=encoder_decoder_model,\n",
    "        src_vocab=cmudict_corpus.src_vocab,\n",
    "        tgt_vocab=cmudict_corpus.tgt_vocab,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "    phoneme_gen_trainer = pl.Trainer(\n",
    "        accelerator=accelerator,\n",
    "        max_epochs=12,\n",
    "        # callbacks=[pl.callbacks.EarlyStopping(monitor='val_cer', mode='min')]\n",
    "    )\n",
    "    phoneme_gen_trainer.fit(model=phoneme_gen_pl_module,\n",
    "                        datamodule=cmudict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pnfpqh1VQGhD",
    "outputId": "0a0d5686-3d0f-4f2b-f4e7-b581340b22b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V AE2 L AH0 N HH AE1 NG AH0 M IY0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can run this cell to test whether you have get your model trained\n",
    "# You are expecting to see a phoneme sequence\n",
    "decode_as_str(\n",
    "    phoneme_gen_pl_module.model.tgt_vocab,\n",
    "    wrapped_generate(\n",
    "        model_to_wrap=phoneme_gen_pl_module.model,\n",
    "        input_ids=encode_as_tensor(phoneme_gen_pl_module.model.src_vocab,\n",
    "                         'v a n l a n i n g h a m ').to(phoneme_gen_pl_module.device)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "f4BHFPWlQGhD",
    "outputId": "e655f5bb-e575-4683-c89a-dd3af121af92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>model-generate-test</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "model-generate-test results: All test cases passed!"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"model-generate-test\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00871fcbab584ce5ae4e29f0f8199fb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "00a53e6e88644a38874457b7268e6927": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "031439a5a65744e99e0fdb9655ad7be2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "033c930004af46128539d3cb0694e33d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f18c03bbbf864437bc5aebb8278d86a8",
      "placeholder": "​",
      "style": "IPY_MODEL_7a2cf8d64a044558aaab5c746d1ba536",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "053d447d9d744c48b608a582859f8e02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d57dd312130445c917c4c35d1865129": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e879ef2db95468a9b7351437ac3ceae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b78ce547b59e4058a5efb06ac54eba3f",
      "placeholder": "​",
      "style": "IPY_MODEL_7408378e21a143d6b736aa02e34c7b87",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "0f34b9ecd14148b58e9dc776d6917b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b87e467d54246cb8d0810763ea16c40",
      "placeholder": "​",
      "style": "IPY_MODEL_ac126008fb4c426780dcddca28537c8f",
      "value": " 63/63 [01:20&lt;00:00,  1.28s/it]"
     }
    },
    "0fe49f11dce94ce095bce682c7142e2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd0b633e00f349aca38e19cfaa83f77b",
       "IPY_MODEL_e5707b91989b4711bc8622b54e54d0c8",
       "IPY_MODEL_0f34b9ecd14148b58e9dc776d6917b30"
      ],
      "layout": "IPY_MODEL_5a19bfc16d434ddbbce44b68c3749764"
     }
    },
    "164b4a43a21b4921b2a1fe5a4edc2570": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18b748d47ef0485bb1a9923794d7e00c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_164b4a43a21b4921b2a1fe5a4edc2570",
      "placeholder": "​",
      "style": "IPY_MODEL_0d57dd312130445c917c4c35d1865129",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "2472f4bff1b745c3858452c2cc79d922": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2529a46a24be479291d2c1c1a533c502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2879f3f5cd84410794b2b7966535d46d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dac030e781e4204a50b7c025acc29ca",
      "placeholder": "​",
      "style": "IPY_MODEL_5e715cefaf2c4520971aad5ca8cc4baa",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "39ef516b8e4f4418a83b7785e1e2d385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_847ef212432e405ba2b15abc24865efb",
      "placeholder": "​",
      "style": "IPY_MODEL_e74e244e7fbd4385a40944f25d962c0e",
      "value": " 2/2 [00:02&lt;00:00,  1.44s/it]"
     }
    },
    "3afbc886957949c0afe68c813c75f48f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45608cd6ddea40728671826eccac2673": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f02b8c4b7dfe4ab4bc21c58376d23d4b",
      "max": 866,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94f75cd5ed664c248b75812669ecec65",
      "value": 866
     }
    },
    "4862785842d64e3e9db5c78c68f3e02e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce99764cfb40433aa86881089d1fbb24",
      "placeholder": "​",
      "style": "IPY_MODEL_2529a46a24be479291d2c1c1a533c502",
      "value": "Epoch 11: 100%"
     }
    },
    "4a239b33f9fb4988a38ea7e670ea237c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50d2c1bf58bd4268bdaca0699b485afa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "519ea72dc41943e9b00dfe21c3920017": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dff2b21aa3cb41b8a2e1531893b587e4",
      "placeholder": "​",
      "style": "IPY_MODEL_eed118af1fe2427f9995d8ba8d521b60",
      "value": " 866/866 [07:46&lt;00:00,  1.86it/s, loss=0.267, v_num=0]"
     }
    },
    "5a19bfc16d434ddbbce44b68c3749764": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "5b87e467d54246cb8d0810763ea16c40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e715cefaf2c4520971aad5ca8cc4baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "651fb14b68a14d0b985d6edbf26f8d93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "659bd89df80d43f4bb5b90d37ffe91ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68f0e413694d4f8fa1c6a0be292d4cdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71d92538baea4082aaa970d81ceddd19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_033c930004af46128539d3cb0694e33d",
       "IPY_MODEL_e2dfef2f341d45329ee016afbc897f1e",
       "IPY_MODEL_a21d5126f2bb444f92e984b7c95e04a1"
      ],
      "layout": "IPY_MODEL_8e15fb69cf01416c80953a8700540485"
     }
    },
    "71dddaa70b7d41f0a2253e3bb46e6867": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7408378e21a143d6b736aa02e34c7b87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "748a44938cb242bb868aeebec8e16942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8e067092f5f4469bf24f99fd6778ed6",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d44ce71756f44604a4874c6c2b0b650e",
      "value": 63
     }
    },
    "7a2cf8d64a044558aaab5c746d1ba536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ae5fb83aa0647ea9a75f5e7d39e3169": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fd9a0405934444ba336e97be71b2afe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8314c02114c042709583f55e64d8e3d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "847ef212432e405ba2b15abc24865efb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "874c4214bddd4b4d82b0516bb2ea0383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8314c02114c042709583f55e64d8e3d1",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da8c72e907394b35ac5621c53d8f46f8",
      "value": 63
     }
    },
    "87937aa39aba43beb25518b72a696a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88b1c6e20208418da874083d41c00252": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2472f4bff1b745c3858452c2cc79d922",
      "placeholder": "​",
      "style": "IPY_MODEL_68f0e413694d4f8fa1c6a0be292d4cdd",
      "value": " 63/63 [01:21&lt;00:00,  1.30s/it]"
     }
    },
    "8e15fb69cf01416c80953a8700540485": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "94f75cd5ed664c248b75812669ecec65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9dac030e781e4204a50b7c025acc29ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a21d5126f2bb444f92e984b7c95e04a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71dddaa70b7d41f0a2253e3bb46e6867",
      "placeholder": "​",
      "style": "IPY_MODEL_00a53e6e88644a38874457b7268e6927",
      "value": " 63/63 [01:19&lt;00:00,  1.27s/it]"
     }
    },
    "a312dfb435d04eeab0ddd93800564bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18b748d47ef0485bb1a9923794d7e00c",
       "IPY_MODEL_748a44938cb242bb868aeebec8e16942",
       "IPY_MODEL_88b1c6e20208418da874083d41c00252"
      ],
      "layout": "IPY_MODEL_c1fd5ef1ddbf4ef6844e619eaeeb439e"
     }
    },
    "a6b95598f61e42d59583830766772da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac126008fb4c426780dcddca28537c8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b62316570fd94b5db329cb31087800a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6b95598f61e42d59583830766772da1",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a239b33f9fb4988a38ea7e670ea237c",
      "value": 2
     }
    },
    "b721507d45fb42e1b7a34ad6feeb49e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fd9a0405934444ba336e97be71b2afe",
      "placeholder": "​",
      "style": "IPY_MODEL_651fb14b68a14d0b985d6edbf26f8d93",
      "value": " 63/63 [01:20&lt;00:00,  1.28s/it]"
     }
    },
    "b78ce547b59e4058a5efb06ac54eba3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba016f47051748048d3ea3eacb040cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4862785842d64e3e9db5c78c68f3e02e",
       "IPY_MODEL_45608cd6ddea40728671826eccac2673",
       "IPY_MODEL_519ea72dc41943e9b00dfe21c3920017"
      ],
      "layout": "IPY_MODEL_031439a5a65744e99e0fdb9655ad7be2"
     }
    },
    "c1fd5ef1ddbf4ef6844e619eaeeb439e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "cd0b633e00f349aca38e19cfaa83f77b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_053d447d9d744c48b608a582859f8e02",
      "placeholder": "​",
      "style": "IPY_MODEL_659bd89df80d43f4bb5b90d37ffe91ec",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "cd47ffdf4ac94160ab9a708c08d0869e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "ce99764cfb40433aa86881089d1fbb24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d44ce71756f44604a4874c6c2b0b650e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8e067092f5f4469bf24f99fd6778ed6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da8c72e907394b35ac5621c53d8f46f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de27f188d4114123bb66e66f195dcbef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e879ef2db95468a9b7351437ac3ceae",
       "IPY_MODEL_b62316570fd94b5db329cb31087800a9",
       "IPY_MODEL_39ef516b8e4f4418a83b7785e1e2d385"
      ],
      "layout": "IPY_MODEL_00871fcbab584ce5ae4e29f0f8199fb1"
     }
    },
    "dff2b21aa3cb41b8a2e1531893b587e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2dfef2f341d45329ee016afbc897f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ae5fb83aa0647ea9a75f5e7d39e3169",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50d2c1bf58bd4268bdaca0699b485afa",
      "value": 63
     }
    },
    "e5707b91989b4711bc8622b54e54d0c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3afbc886957949c0afe68c813c75f48f",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87937aa39aba43beb25518b72a696a12",
      "value": 63
     }
    },
    "e74e244e7fbd4385a40944f25d962c0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eed118af1fe2427f9995d8ba8d521b60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f02b8c4b7dfe4ab4bc21c58376d23d4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f18c03bbbf864437bc5aebb8278d86a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fba1cce16ccf420487e03472b79f212a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2879f3f5cd84410794b2b7966535d46d",
       "IPY_MODEL_874c4214bddd4b4d82b0516bb2ea0383",
       "IPY_MODEL_b721507d45fb42e1b7a34ad6feeb49e2"
      ],
      "layout": "IPY_MODEL_cd47ffdf4ac94160ab9a708c08d0869e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
