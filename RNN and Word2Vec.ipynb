{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybDNhhfgHf3V",
    "outputId": "32011896-f44b-4277-df77-f97738279bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-27 19:03:53--  https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw2-files/student/required_files.zip\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/assignments/hw2-files/student/required_files.zip [following]\n",
      "--2022-10-27 19:03:53--  https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/assignments/hw2-files/student/required_files.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6863 (6.7K) [application/zip]\n",
      "Saving to: ‘required_files.zip’\n",
      "\n",
      "required_files.zip  100%[===================>]   6.70K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-27 19:03:53 (57.2 MB/s) - ‘required_files.zip’ saved [6863/6863]\n",
      "\n",
      "Archive:  required_files.zip\n",
      "  inflating: .DS_Store               \n",
      "  inflating: requirements.txt        \n",
      "   creating: tests/\n",
      "  inflating: tests/rnnnlm-lyric-gen-greedy-decoding.py  \n",
      "  inflating: tests/rnnnlm-lyric-gen-improved-model.py  \n",
      "  inflating: tests/rnnnlm-lyric-gen-vanilla-rnn-impl.py  \n",
      "  inflating: tests/rnnnlm-lyric-gen-sampling-decoding.py  \n",
      "  inflating: tests/rnnnlm-lyric-gen-dataset-impl.py  \n",
      "  inflating: tests/word2vec-analogy-impl.py  \n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: datascience in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.17.5)\n",
      "Requirement already satisfied: jupyter_client in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (6.1.12)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (5.3.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.3.5)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.7.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.11.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (5.6.1)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (5.7.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.3.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.21.6)\n",
      "Collecting otter-grader==4.0.1\n",
      "  Downloading otter_grader-4.0.1-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 4.9 MB/s \n",
      "\u001b[?25hCollecting pdfkit\n",
      "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-2.11.1-py3-none-any.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 43.2 MB/s \n",
      "\u001b[?25hCollecting wkhtmltopdf\n",
      "  Downloading wkhtmltopdf-0.2.tar.gz (9.7 kB)\n",
      "Collecting overrides===6.2.0\n",
      "  Downloading overrides-6.2.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: torch~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.12.1+cu113)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.13.1+cu113)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (0.12.1+cu113)\n",
      "Collecting pytorch-lightning~=1.7.7\n",
      "  Downloading pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
      "\u001b[K     |████████████████████████████████| 708 kB 34.7 MB/s \n",
      "\u001b[?25hCollecting gensim~=4.2.0\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 191 kB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (1.0.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (6.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (1.15.0)\n",
      "Collecting jupytext\n",
      "  Downloading jupytext-1.14.1-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 49.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (2.23.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (7.1.2)\n",
      "Collecting fica>=0.2.0\n",
      "  Downloading fica-0.2.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (0.4.6)\n",
      "Collecting python-on-whales\n",
      "  Downloading python_on_whales-0.53.0-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (1.14.1)\n",
      "Requirement already satisfied: gspread in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 15)) (1.12.11)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.12.1->-r requirements.txt (line 23)) (4.1.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2022.10.0)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2.9.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (21.3)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.10.1-py3-none-any.whl (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 46.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.64.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=4.2.0->-r requirements.txt (line 29)) (5.2.1)\n",
      "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.8.6)\n",
      "Requirement already satisfied: docutils in /usr/local/lib/python3.7/dist-packages (from fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.17.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.8.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.0.9)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.37.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.17.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.50.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.3.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (57.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (1.8.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (4.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (3.9.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 26)) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->otter-grader==4.0.1->-r requirements.txt (line 15)) (3.2.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (5.5.0)\n",
      "Requirement already satisfied: folium>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (0.12.1.post1)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (7.9.0)\n",
      "Requirement already satisfied: branca in /usr/local/lib/python3.7/dist-packages (from datascience->-r requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->-r requirements.txt (line 10)) (2.0.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (4.11.2)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (4.4.2)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (4.8.0)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 44.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->datascience->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->datascience->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 5)) (2022.5)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (3.6.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (5.5.0)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (5.0.1)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->-r requirements.txt (line 12)) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->-r requirements.txt (line 12)) (4.3.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 12)) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 12)) (5.10.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 24)) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 30)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 30)) (1.2.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.0.4)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.17.4)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.31.6)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (3.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.56.4)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from jupytext->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.10.2)\n",
      "Collecting markdown-it-py<3.0.0,>=1.0.0\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 3.4 MB/s \n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->datascience->-r requirements.txt (line 1)) (8.1.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from python-on-whales->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.10.2)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from python-on-whales->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.4.2)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.10.3)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.4.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (0.7.12)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.2.4)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 15)) (1.1.5)\n",
      "Building wheels for collected packages: sklearn, wkhtmltopdf\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=fcc930adb8b477fb592ddd6710a887eb42f708b3981779cdc3a60ae031cc27b1\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "  Building wheel for wkhtmltopdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wkhtmltopdf: filename=wkhtmltopdf-0.2-py3-none-any.whl size=11149 sha256=d6898d8162bd10cc5ad5f818e2d15c81d05e526a103ff12585c061bb2b80c5f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/15/f3/c2f2199ba236c73e7e14888e5e5e5ff6da2fb13d1f26072a9e\n",
      "Successfully built sklearn wkhtmltopdf\n",
      "Installing collected packages: jedi, mdurl, markdown-it-py, mdit-py-plugins, torchmetrics, python-on-whales, pyDeprecate, jupytext, fica, wkhtmltopdf, sklearn, pytorch-lightning, PyPDF2, pdfkit, overrides, otter-grader, gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed PyPDF2-2.11.1 fica-0.2.0 gensim-4.2.0 jedi-0.18.1 jupytext-1.14.1 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 otter-grader-4.0.1 overrides-6.2.0 pdfkit-1.0.0 pyDeprecate-0.3.2 python-on-whales-0.53.0 pytorch-lightning-1.7.7 sklearn-0.0 torchmetrics-0.10.1 wkhtmltopdf-0.2\n"
     ]
    }
   ],
   "source": [
    "# Downloads required packages and files\n",
    "required_files = \"https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw2-files/student/required_files.zip\"\n",
    "! wget $required_files && unzip -o required_files.zip\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5PyZIC41Hf3Z"
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "\n",
    "grader = otter.Notebook(colab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLm_JxCxrkIE"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "In the last few lectures, you have looked at two work-horses of natural language processing: recurrent neural networks, and word embeddings (distributional semantics). In this assignment, you will put the learning into practice by implementing some of these yourself, and doing some exploratory analysis.\n",
    "\n",
    "# Setup\n",
    "\n",
    "For this assignment, as in the previous one, we will be using Google Colab, for both code as well as descriptive questions. Your task is to finish all the questions in the Colab notebook and then upload a PDF version of the notebook, and a viewable link on Gradescope. \n",
    "\n",
    "### Google colaboratory\n",
    "\n",
    "Before getting started, get familiar with google colaboratory:\n",
    "https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "\n",
    "This is a neat python environment that works in the cloud and does not require you to\n",
    "set up anything on your personal machine\n",
    "(it also has some built-in IDE features that make writing code easier).\n",
    "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
    "with other people.\n",
    "\n",
    "__Note:__\n",
    "1. You may need to change your Runtime setting to GPU in order to run the following code blocks.\n",
    "2. On changing the Runtime setting, you would be required to run the previous code-blocks again.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Before you start working on this homework do the following steps:\n",
    "\n",
    "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
    "2. Follow all the steps in this collaboratory file and write / change / uncomment code as necessary.\n",
    "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
    "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected. Copy the link and paste it in the box below.\n",
    "5. After completing the notebook, press __File > Download .ipynb__ to download a local copy on your computer, and then upload the file to Gradescope.\n",
    "\n",
    "__Special handling for model checkpoints.__\n",
    "6. As the homework requires training neural models, such trained model checkpoints should also be submitted together with the notebook, hence avoiding re-training during the grading phase. For such model checkpoints, they would be stored at `./lightning_logs` directory. You have to first locate the directory from the left side panel (`Files`) on Colab.\n",
    "7. Enter `./lightning_logs` and find the training label that you would like to submit. The versions are labelled with respect to the training calls.\n",
    "8. Download the `.ckpt` file from `./lightning_logs/<your_version>/checkpoints/<name>.ckpt`.\n",
    "9. Rename the downloaded checkpoint and re-name it as the corresponding name shown in the question, say `vanilla_rnn_model.ckpt`.\n",
    "10. Submit checkpoint file(s) together with your notebook to the autograder. Please make sure that checkpoint files should be put at the same directory level as the notebook (content root).\n",
    "\n",
    "\n",
    "\n",
    "__Paste your notebook link in the box below.__ _(0 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fcLCcR8WNay-"
   },
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/drive/10wCNs1U1pIN4fwhsfYRFBzCLApeV3v7F?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2kR0zyxKUh2"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Paste your Colab notebook link here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_zHfzMzMHf3c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "\n",
    "import torch\n",
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import gensim.downloader as gensim_api\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBvH5tz5Hf3c",
    "outputId": "d9416c10-2224-42b3-8484-cc61162a3d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is running for \"student\".\n",
      "Students should make sure you are running under the \"student\" mode.\n",
      "You are using \"cpu\".\n"
     ]
    }
   ],
   "source": [
    "# Checks whether it is in the autograder grading mode\n",
    "# Checks whether GPU accelerators are available\n",
    "is_autograder = os.path.exists('is_autograder.py')\n",
    "if torch.cuda.is_available() and not is_autograder:\n",
    "    accelerator = 'gpu'\n",
    "else:\n",
    "    accelerator = 'cpu'\n",
    "print(f'The notebook is running for \"{\"autograder\" if is_autograder else \"student\"}\".')\n",
    "print('Students should make sure you are running under the \"student\" mode.')\n",
    "print(f'You are using \"{accelerator}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1lkJaVNHf3c",
    "outputId": "3f3903c3-d56c-4dde-b98c-ac6aa812ec02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed everything to make sure all experiments are reproducible\n",
    "pl.seed_everything(seed=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DfWhg6LRHf3d"
   },
   "outputs": [],
   "source": [
    "HOMEWORK_DATA_URL = \"https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw2-files/student/\"\n",
    "\n",
    "SONGDATA_TRAIN_PATH = 'songdata_train.csv'\n",
    "SONGDATA_VAL_PATH = 'songdata_val.csv'\n",
    "\n",
    "VANILLA_RNN_MODEL_CKPT_PATH = 'vanilla_rnn_model.ckpt'\n",
    "IMPROVED_MODEL_CKPT_PATH = 'improved_model.ckpt'\n",
    "\n",
    "# We are using the same label for both padding and OOV\n",
    "# This leads to the same behavior for loss computation\n",
    "PADDING_LABEL = 100\n",
    "OOV_LABEL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gAinSAw6Hf3d"
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    def _download(url: str, filename: str) -> str:\n",
    "        txt = urllib.request.urlopen(url)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(txt.read().decode('utf-8'))\n",
    "\n",
    "    _download(f'{HOMEWORK_DATA_URL}/{SONGDATA_TRAIN_PATH}', SONGDATA_TRAIN_PATH)\n",
    "    _download(f'{HOMEWORK_DATA_URL}/{SONGDATA_VAL_PATH}', SONGDATA_VAL_PATH)\n",
    "\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj0UWhFRmNcm"
   },
   "source": [
    "### Part 1: Language modeling with RNNs\n",
    "\n",
    "For the first part of this assignment, you will train a character-level language model on a lyrics dataset.\n",
    "\n",
    "More specifically, you will:\n",
    "1. Implement a character-level language model based on recurrent neural networks.\n",
    "2. Train it on a lyrics dataset.\n",
    "3. Sample previously unseen lyrics from our model.\n",
    "4. Augment your model with an artist information, so that you can generate songs conditioned on some artist.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "#### Character-Level language model\n",
    "\n",
    "![alt text](http://warmspringwinds.github.io/assets/img/character_level_model.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Before choosing a model, let’s have a closer look at our task. Given current letter and all previous letters, we will try to predict the next character. During training we will just take a sequence, and use all its characters except the last one as an input and the same sequence starting from the second character as groundtruth (see the picture above).\n",
    "\n",
    "Our language model is defined on a character level. We will create a dictionary which will contain all English characters plus some special symbols, such as period, comma, and end-of-line symbol. Each charecter will be represented as one-hot-encoded tensor. For more information about character-level models and examples, refer to the [the following resource](https://github.com/spro/practical-pytorch).\n",
    "\n",
    "\n",
    "Our objective is to model is _p_(current letter | all previous letters). At first, the task seems intractable as the number of previous letters is variable and it might become really large in case of long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb9fEwMBHf3e"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ Based on what you learned in Assignment 1, describe how you would deal with this problem? _(3 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IDdwQ-aubA2"
   },
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtlXvJGxHf3f"
   },
   "source": [
    "1. based on Markov Assumption, we can simpilfy this problem to n-gram program.\n",
    "2. we can solve it by using recurrent neural networks, because this model can handle inputs of variant lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7b0RJuauGLd"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "![alt text](http://warmspringwinds.github.io/assets/img/rnn_unfold.jpg \"Logo Title Text 1\")\n",
    "\n",
    "RNNs are a family of neural networks for processing sequential data. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Because of arbitrary size input sequences, they are concisely depicted as a graph with a cycle (see the picture). But they can be “unfolded” if the size of input sequence is known. They define a non-linear mapping from a current input $x_t$ and previous hidden state $s_{t−1}$ to the output $o_t$ and current hidden state $s_t$. Hidden state size has a predefined size and stores features which are updated on each step and affect the result of mapping.\n",
    "\n",
    "Now align the previous picture of the character-level language model and the unfolded RNN picture to see how we are using the RNN model to learn a character level language model.\n",
    "\n",
    "While the picture depicts the Vanilla RNN, LSTMs or GRUs are more commonly used in practice.\n",
    "\n",
    "For a more elaborate introduction to RNNs, we refer reader to [the following resource](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8neiuy1SHf3f"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ Why are LSTMs/GRUs preferred over RNNs? _(3 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvoUw3D7yG0r"
   },
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eobcANyqHf3g"
   },
   "source": [
    "1. Because there are some gates in LSTM and GRU that can control the memory block, so it can choose to keep some long-term but usful information and discard some unimportant one. \n",
    "2. LSTM/GRU can help to reduce vanish gradient problem in RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxyGXlYGnkuK"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Downloading the data\n",
    "\n",
    "For your experiments, you will use a subset of [Song Lyrics Kaggle dataset](https://www.kaggle.com/datasets/deepshah16/song-lyrics-dataset) which contains a good variety of recent artists and more older ones. It is stored as a Pandas file originally, but we will provide you with a Python wrapper to use the data for training your models easily.\n",
    "\n",
    "The dataset is originally split to different csv files. In the homework, we provided a concatenated version for easy processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q4EfBmmox5J"
   },
   "source": [
    "#### Exploring the downloaded data\n",
    "\n",
    "First, let us create a dictionary, we will use 100 characters and some special symbols including $\\n$, which will allow our generator to decide when to move to a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yp6UPpBRpCt1"
   },
   "outputs": [],
   "source": [
    "all_characters = string.printable\n",
    "number_of_characters = len(all_characters)\n",
    "\n",
    "#print(len(all_characters))\n",
    "#print(all_characters[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8mlo_01pHs8"
   },
   "source": [
    "Below we will define some helper functions that will help us to convert a character to corresponding\n",
    "labels that we will use for actual training. Read the implementations and figure out what each function does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9Nx7sSyHf3h"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ Why would a `pad_sequence` function be required? _(2 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8T5nCQ1Hf3h"
   },
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aIz5fLKHf3h"
   },
   "source": [
    "because if we want to train a batch of sequence, and to do matrix manipulation, these sequence need to have the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPXl-vuCHf3i"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JFnPzBRtpMYM"
   },
   "outputs": [],
   "source": [
    "def character_to_label(character: str, oov_label: int = OOV_LABEL) -> int:\n",
    "    \"\"\"Maps a character to an int from the vocabulary.\"\"\"\n",
    "    character_label = all_characters.find(character)\n",
    "    if character_label == -1:  # Not found in the vocab\n",
    "        return oov_label\n",
    "    return character_label\n",
    "\n",
    "\n",
    "def string_to_labels(character_string: str, oov_label: int = OOV_LABEL) -> List[int]:\n",
    "    \"\"\"Maps a string to a list of int as a label sequence.\"\"\"\n",
    "    return list(map(lambda character: character_to_label(character, oov_label=oov_label), character_string))\n",
    "\n",
    "\n",
    "def token_ids_to_str(token_ids: List[int]) -> str:\n",
    "    return ''.join([all_characters[x] for x in token_ids])\n",
    "\n",
    "\n",
    "def pad_sequence(seq: torch.Tensor, max_length: int, pad_label: int = PADDING_LABEL) -> torch.Tensor:\n",
    "    \"\"\"Pads a sequence to the maximum length with a padding label.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : torch.Tensor\n",
    "        A tensor represents a sequence in the shape of (seq_len)\n",
    "    max_length : int\n",
    "        If the sequence is shorter than this, pad it to this length.\n",
    "    pad_label : int\n",
    "        The label used to pad.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_seq : torch.Tensor\n",
    "        The padded sequence in the shape of (max_length)\n",
    "    \"\"\"\n",
    "    current_len = seq.shape[-1]\n",
    "    assert current_len <= max_length, f'The current length {current_len} is already larger than the max_length={max_length}.'\n",
    "    if current_len == max_length:\n",
    "        return seq\n",
    "    padded_seq = torch.cat(\n",
    "        [\n",
    "            seq,\n",
    "            torch.tensor([pad_label], dtype=seq.dtype, device=seq.device).expand(*seq.shape[:-1],\n",
    "                                                                                 max_length - current_len)\n",
    "        ], dim=-1)\n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSVcI3QSHf3i"
   },
   "source": [
    "Now we will define a dataset class that will take care of loading data from the dataset. Complete the `__getitem__` function below. Specifically, your job is to obtain input-output pairs of sequences from the data.\n",
    "\n",
    "Then, you will have to implement a `collate_fn()` in the `LyricsGenerationDataModule` class, which wraps PyTorch datasets to be a PyTorch Lightning DataModule that describes how data is loaded for different splits. _(6 points)_\n",
    "\n",
    "__Note:__\n",
    "Why padding? PyTorch is the framework that does tensor computation with automatic differentiation support. When dealing with sequences in different lengths, we have to pad them to the same length, namely the maximum length, to construct a tensor. Imaging that you have a list of vectors with different lengths, you cannot directly put them together to get a matrix; instead, you have to first pad them with some value (padding value) to make them in the same length, and then concate them to get a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gNGk1TS_Hf3i"
   },
   "outputs": [],
   "source": [
    "# Here are the artists\n",
    "ARTISTS = [\n",
    "    'Dua Lipa',\n",
    "    'Ariana Grande',\n",
    "    'Charlie Puth',\n",
    "    'Drake',\n",
    "    'BTS (방탄소년단)',\n",
    "    'Billie Eilish',\n",
    "    'Cardi B',\n",
    "    'Eminem',\n",
    "    'Lady Gaga',\n",
    "    'Nicki Minaj',\n",
    "    'Beyoncé',\n",
    "    'Maroon 5',\n",
    "    'Ed Sheeran',\n",
    "    'Justin Bieber',\n",
    "    'Taylor Swift',\n",
    "    'Selena Gomez',\n",
    "    'Coldplay',\n",
    "    'Rihanna',\n",
    "    'Katy Perry',\n",
    "    'Post Malone',\n",
    "    'Khalid'\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LyricsGenerationTensor:\n",
    "    \"\"\"Describes input instance/batch to the model.\"\"\"\n",
    "    input_sequence: torch.Tensor\n",
    "    output_sequence: torch.Tensor\n",
    "    sequence_length: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jxryOy3AqPDL"
   },
   "outputs": [],
   "source": [
    "class LyricsGenerationDataset(Dataset):\n",
    "    \"\"\"A dataset class that reads the song dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 csv_file_path: str,\n",
    "                 minimum_song_count: Optional[int] = None,\n",
    "                 artists: Optional[List[str]] = None,\n",
    "                 oov_label: int = OOV_LABEL):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        csv_file_path : str\n",
    "            The path to the dataset CSV file.\n",
    "        minimum_song_count : int, optional\n",
    "            The default is None, which means not filtering.\n",
    "            Filter artists that do not have at least the minimum number of songs.\n",
    "        artists : List[str], optional\n",
    "            The default is None, which means not filtering.\n",
    "            Filter artists by their names.\n",
    "        oov_label : int, optional\n",
    "            The default is OOV_LABEL. The label for out of the vocabulary label.\n",
    "        \"\"\"\n",
    "        raw_dataframe = pd.read_csv(csv_file_path)\n",
    "        # Filters out NaN\n",
    "        raw_dataframe = raw_dataframe[~ raw_dataframe.Lyric.isnull()]\n",
    "        # Filters out length=1 as they would result in length=0 in the training\n",
    "        self.lyrics_dataframe = raw_dataframe[raw_dataframe.Lyric.str.len() > 1]\n",
    "\n",
    "        if artists:\n",
    "            self.lyrics_dataframe = self.lyrics_dataframe[self.lyrics_dataframe.Artist.isin(artists)]\n",
    "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
    "\n",
    "        if minimum_song_count:\n",
    "            self.lyrics_dataframe = self.lyrics_dataframe.groupby('Artist').filter(\n",
    "                lambda x: len(x) > minimum_song_count)\n",
    "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
    "\n",
    "        self.artists_list = list(self.lyrics_dataframe.Artist.unique())\n",
    "        self.number_of_artists = len(self.artists_list)\n",
    "        self.oov_label = oov_label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of instances read in the dataset.\"\"\"\n",
    "        return len(self.lyrics_dataframe)\n",
    "\n",
    "    def __getitem__(self, index: int) -> LyricsGenerationTensor:\n",
    "        assert index < len(self), f'{index} exceeds the dataset length.'\n",
    "\n",
    "        sequence_raw_string: str = self.lyrics_dataframe.loc[index].Lyric\n",
    "        sequence_string_labels: List[int] = string_to_labels(sequence_raw_string, oov_label=self.oov_label)\n",
    "        sequence_length: int = len(sequence_string_labels) - 1\n",
    "\n",
    "        # Generate input and output sequence (shifted by 1 character)\n",
    "        # from the sequence_string_labels\n",
    "        # TODO: Your code here\n",
    "        input = torch.tensor(sequence_string_labels[:-1],dtype = torch.long)\n",
    "        output = torch.tensor(sequence_string_labels[1:],dtype = torch.long)\n",
    "        length = torch.tensor([sequence_length],dtype = torch.long)\n",
    "        return LyricsGenerationTensor(input,output,length)\n",
    "        # Returns a `LyricsGenerationInstance`\n",
    "        # TODO: Your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JiKvpedMHf3j"
   },
   "outputs": [],
   "source": [
    "class LyricsGenerationDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Wraps PyTorch dataset as a lightning data module.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_paths: Dict[str, str],\n",
    "                 artists: List[str],\n",
    "                 batch_size: int = 32,\n",
    "                 shuffle: bool = True,\n",
    "                 padding_label: int = PADDING_LABEL,\n",
    "                 oov_label: int = OOV_LABEL):\n",
    "        super(LyricsGenerationDataModule, self).__init__()\n",
    "        self.datasets: Dict[str, Dataset] = {\n",
    "            k: LyricsGenerationDataset(csv_file_path=v, artists=artists, oov_label=oov_label)\n",
    "            for k, v in dataset_paths.items()\n",
    "            if k in ('train', 'val', 'test')\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.padding_label = padding_label\n",
    "        self.oov_label = oov_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(padding_label: int, instances: List[LyricsGenerationTensor]) -> LyricsGenerationTensor:\n",
    "        \"\"\"mai a list of instances and composes a batch.\n",
    "\n",
    "        The function does the following things:\n",
    "        1. Figure out the maximum length within a batch. Padding means that\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        padding_label : int\n",
    "            The label is to be used to pad sequences.\n",
    "        instances : List[LyricsGenerationTensor]\n",
    "            A list of `LyricsGenerationTensor` to comprise.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch : LyricsGenerationTensor\n",
    "            A single `LyricsGenerationTensor` where tensors are batched instances.\n",
    "        \"\"\"\n",
    "        # Pad sequences so that all of them have the same length. For\n",
    "        # the output sequence, use pad_label=self.padding_label so that it is omitted\n",
    "        # in cross-entropy loss --- and can be accessed later when creating the loss func.\n",
    "        # TODO: Your code here\n",
    "        lengths = [int(instance.sequence_length) for instance in instances]\n",
    "        max_length = max(lengths)\n",
    "        input = [pad_sequence(instance.input_sequence,max_length,padding_label) for instance in instances]\n",
    "        output = [pad_sequence(instance.output_sequence,max_length,padding_label) for instance in instances]\n",
    "\n",
    "        padded_instances = LyricsGenerationTensor(torch.stack(input),torch.stack(output),torch.tensor(lengths,dtype = torch.long))\n",
    "        \n",
    "        return padded_instances\n",
    "        #pad_sequence(seq: torch.Tensor, max_length: int, pad_label: int = PADDING_LABEL) -> torch.Tensor:\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.datasets['train'],\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=self.shuffle,\n",
    "                          collate_fn=lambda x: self.collate_fn(self.padding_label, x))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.datasets['val'],\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: self.collate_fn(self.padding_label, x))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.datasets['test'],\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: self.collate_fn(self.padding_label, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KHonZx8IHf3k"
   },
   "outputs": [],
   "source": [
    "lyrics_gen_data_module = LyricsGenerationDataModule(\n",
    "    dataset_paths={'train': SONGDATA_TRAIN_PATH, 'val': SONGDATA_VAL_PATH},\n",
    "    artists=ARTISTS,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "YHfdTBlCHf3k",
    "outputId": "672354c0-4af5-47da-ac06-e57034fb1ebc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0716bc91-cead-4051-a983-d26c22a7de10\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Album</th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>New Rules</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>one one one one one   talkin' in my sleep at n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>IDGAF</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>you call me all friendly tellin' me how much y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Blow Your Mind (Mwah)</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>i know it's hot i know we've got something tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Be the One</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015-10-30</td>\n",
       "      <td>i see the moon i see the moon i see the moon o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Break My Heart</td>\n",
       "      <td>Future Nostalgia</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>i've always been the one to say the first good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>4252</td>\n",
       "      <td>6021</td>\n",
       "      <td>Khalid</td>\n",
       "      <td>Young dumb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>so you're still thinking of me just like i kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>4253</td>\n",
       "      <td>6022</td>\n",
       "      <td>Khalid</td>\n",
       "      <td>Khalid - Vertigo  (Tradução Português)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>será que é melhor apenas acreditar nas teorias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>4254</td>\n",
       "      <td>6023</td>\n",
       "      <td>Khalid</td>\n",
       "      <td>Better (Miles Away Remix)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>i'm not really drunk i never get that fucked u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>4255</td>\n",
       "      <td>6024</td>\n",
       "      <td>Khalid</td>\n",
       "      <td>Khalid - Better (Official Music Video)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-05-07</td>\n",
       "      <td>users considering it's a virus or malware must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>4256</td>\n",
       "      <td>6025</td>\n",
       "      <td>Khalid</td>\n",
       "      <td>Perfect Lover</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lyrics for this song have yet to be released p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4226 rows × 8 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0716bc91-cead-4051-a983-d26c22a7de10')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0716bc91-cead-4051-a983-d26c22a7de10 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0716bc91-cead-4051-a983-d26c22a7de10');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      index  Unnamed: 0    Artist                                   Title  \\\n",
       "0         0           0  Dua Lipa                               New Rules   \n",
       "1         1           2  Dua Lipa                                   IDGAF   \n",
       "2         2           3  Dua Lipa                   Blow Your Mind (Mwah)   \n",
       "3         3           4  Dua Lipa                              Be the One   \n",
       "4         4           5  Dua Lipa                          Break My Heart   \n",
       "...     ...         ...       ...                                     ...   \n",
       "4221   4252        6021    Khalid                              Young dumb   \n",
       "4222   4253        6022    Khalid  Khalid - Vertigo  (Tradução Português)   \n",
       "4223   4254        6023    Khalid               Better (Miles Away Remix)   \n",
       "4224   4255        6024    Khalid  Khalid - Better (Official Music Video)   \n",
       "4225   4256        6025    Khalid                           Perfect Lover   \n",
       "\n",
       "                 Album  Year        Date  \\\n",
       "0             Dua Lipa  2017  2017-06-02   \n",
       "1             Dua Lipa  2017  2017-06-02   \n",
       "2             Dua Lipa  2016  2016-08-26   \n",
       "3             Dua Lipa  2015  2015-10-30   \n",
       "4     Future Nostalgia  2020  2020-03-25   \n",
       "...                ...   ...         ...   \n",
       "4221               NaN  2017  2017-02-02   \n",
       "4222               NaN  2018  2018-10-28   \n",
       "4223               NaN  2018  2018-12-12   \n",
       "4224               NaN  2018  2018-05-07   \n",
       "4225               NaN   NaN         NaN   \n",
       "\n",
       "                                                  Lyric  \n",
       "0     one one one one one   talkin' in my sleep at n...  \n",
       "1     you call me all friendly tellin' me how much y...  \n",
       "2     i know it's hot i know we've got something tha...  \n",
       "3     i see the moon i see the moon i see the moon o...  \n",
       "4     i've always been the one to say the first good...  \n",
       "...                                                 ...  \n",
       "4221  so you're still thinking of me just like i kno...  \n",
       "4222  será que é melhor apenas acreditar nas teorias...  \n",
       "4223  i'm not really drunk i never get that fucked u...  \n",
       "4224  users considering it's a virus or malware must...  \n",
       "4225  lyrics for this song have yet to be released p...  \n",
       "\n",
       "[4226 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us inspect the dataset quickly\n",
    "lyrics_gen_data_module.datasets['train'].lyrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "GeLVL-OnyFvg",
    "outputId": "6746e296-336d-4b1e-c4b3-2462d37ede94"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen_dm = LyricsGenerationDataModule(\n",
    "                dataset_paths={'train': SONGDATA_TRAIN_PATH, 'val': SONGDATA_VAL_PATH},\n",
    "                artists=ARTISTS,\n",
    "                batch_size=32,\n",
    "                shuffle=False\n",
    "            )\n",
    "first_instance = test_gen_dm.datasets['train'][8]\n",
    "#print(first_instance.input_sequence)\n",
    "#print(first_instance.output_sequence)\n",
    "#print(first_instance.sequence_length)\n",
    "first_instance.input_sequence.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "fCf8fuhMzaDQ",
    "outputId": "43b11a5a-c180-47d4-e5fd-d2a801a641b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_dataset: Dataset = lyrics_gen_data_module.datasets['train']\n",
    "instances: List[LyricsGenerationTensor] = [train_dataset[i] for i in range(batch_size)]\n",
    "batch: LyricsGenerationTensor = LyricsGenerationDataModule.collate_fn(padding_label=-100, instances=instances)\n",
    "max_len = max([x.sequence_length.item() for x in instances])\n",
    "\n",
    "#for i, ins in enumerate(instances):\n",
    "i=0\n",
    "ins = instances[0]\n",
    "seq_len: int = ins.sequence_length.item()\n",
    "#print(seq_len)\n",
    "#print(ins.input_sequence)\n",
    "#print(batch.input_sequence[i][1] )\n",
    "batch.input_sequence[i][1].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "pSUHjJ09Hf3l",
    "outputId": "19c3cadb-e567-4691-cfc8-4a2b618b1635"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>rnnnlm-lyric-gen-dataset-impl</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "rnnnlm-lyric-gen-dataset-impl results: All test cases passed!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"rnnnlm-lyric-gen-dataset-impl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4krJ8S-jsub7"
   },
   "source": [
    "#### Implementing an RNN class\n",
    "\n",
    "First, we will implement a __vanilla RNN__ using PyTorch. Fill in the code block below to complete the implementation. _(8 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "f3oH_06ms5sF"
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int = 101,\n",
    "                 hidden_size: int = 512,\n",
    "                 num_classes: int = 100,\n",
    "                 n_layers: int = 2,\n",
    "                 padding_label: int = PADDING_LABEL):\n",
    "        # input_size = 101 -- 100 characters + background character\n",
    "        # num_classes = 100 -- we predict what character goes next\n",
    "        super(VanillaRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.padding_label = padding_label\n",
    "\n",
    "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
    "        # Below use nn.Embedding to map from input_size to hidden_size\n",
    "        # nn.Embedding converts labels into one-hot encoding and runs a linear\n",
    "        # layer on each of the converted one-hot encoded elements\n",
    "        # TODO: Your code here\n",
    "        self.embeddings = nn.Embedding(self.input_size, self.hidden_size, self.padding_label)        \n",
    "\n",
    "        # Below use nn.RNN that accepts hidden_size as input size,\n",
    "        # and has hidden size equal to hidden_size argument and n_layers\n",
    "        # You have to set `batch_first=True` to make it compatible with\n",
    "        # the `forward()` method.\n",
    "        # TODO: Your code here\n",
    "        self.rnn = nn.RNN(self.hidden_size,self.hidden_size , self.n_layers,batch_first=True)\n",
    "\n",
    "        # Below use nn.Linear to make representation of hidden_size\n",
    "        # to the number of classes that will be fed to softmax\n",
    "        # to decide which character goes next\n",
    "        # TODO: Your code here\n",
    "        self.ffnn = nn.Linear(self.hidden_size,self.num_classes)\n",
    "\n",
    "    def forward(self,\n",
    "                input_sequences: torch.Tensor,\n",
    "                input_sequences_lengths: torch.Tensor,\n",
    "                hidden: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Runs the forward pass for the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sequences : torch.Tensor\n",
    "            A batch of padded input sequences in the shape of\n",
    "            (batch_size, sequence_length)\n",
    "        input_sequences_lengths : torch.Tensor\n",
    "            A batch of sequence lengths in the shape of (batch_size)\n",
    "        hidden : torch.Tensor, optional\n",
    "            A batch of hidden states in the shape of\n",
    "            (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits_flatten : torch.Tensor\n",
    "            A batch of logits in the shape of\n",
    "            (batch_size, sequence_length, num_classes)\n",
    "        updated_hidden : torch.Tensor\n",
    "            A batch of updated hidden states in the shape of\n",
    "            (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        \"\"\"\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        embedded = self.embeddings(input_sequences)\n",
    "\n",
    "        # This is needed for efficient processing of sequences of\n",
    "        # variable lengths. Feel free to skip\n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        packed: PackedSequence = torch.nn.utils.rnn.pack_padded_sequence(embedded,\n",
    "                                                                         input_sequences_lengths.cpu(),\n",
    "                                                                         batch_first=True,\n",
    "                                                                         enforce_sorted=False)\n",
    "\n",
    "        # `outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        # `updated_hidden` shape: (num_layers, batch_size, hidden_size)\n",
    "        outputs, updated_hidden = self.rnn(packed, hidden)\n",
    "        # `unpacked_outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        # `output_lengths` shape: (batch_size)\n",
    "        # Unpacks (back to padded)\n",
    "        unpacked_outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs,\n",
    "                                                                                  batch_first=True)\n",
    "\n",
    "        # Shape: (batch_size, sequence_length, num_classes)\n",
    "        logits = self.ffnn(unpacked_outputs)\n",
    "        \n",
    "        return logits, updated_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "r83Y5yDYHf3n",
    "outputId": "2ae14b93-5338-44a0-97b6-13acfbaceb24"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>rnnnlm-lyric-gen-vanilla-rnn-impl</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "rnnnlm-lyric-gen-vanilla-rnn-impl results: All test cases passed!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"rnnnlm-lyric-gen-vanilla-rnn-impl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2Fkm2DCHf3n"
   },
   "source": [
    "# Training a Vanilla RNN\n",
    "\n",
    "In this part, you would train a Vanilla RNN model that you just implemented above. In order to leverage PyTorch Lightning training loop, we have to first wrap the implemented model with a `LightningModule`. We designate one lightning module as a task that defines different aspects of training, e.g. training steps, validation steps, and loss computation, etc.,. With this design, as long as we are doing the same task, we can flexibly plug-in different model architectures to have them trained under the same formulation. Here, we start with training the lyrics generation task using the `VanillaRNN` implemented above.\n",
    "\n",
    "For the detailed information about Lightning, please refer to its [official documentations](https://pytorch-lightning.readthedocs.io/en/stable/model/train_model_basic.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "czB7Y9FwHf3n"
   },
   "outputs": [],
   "source": [
    "class LyricsGenerationTask(pl.LightningModule):\n",
    "    \"\"\"Wraps a PyTorch module as a Lightning Module for the lyrics generation task.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001, padding_label: int = PADDING_LABEL):\n",
    "        super(LyricsGenerationTask, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        # Please make sure `ignore_index` is consistent with your `padding_label`.\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=padding_label)\n",
    "\n",
    "    def training_step(self, batch: LyricsGenerationTensor) -> torch.Tensor:\n",
    "        \"\"\"Defines the training step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : LyricsGenerationTensor\n",
    "            The batched training instances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            The loss computed using CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        logits, _ = self.model(batch.input_sequence, batch.sequence_length)\n",
    "        flattened_output_sequence = batch.output_sequence.view(-1)\n",
    "        # This is needed for cross entropy loss\n",
    "        # Shape: (batch_size * sequence_length, num_classes)\n",
    "        logits_flatten = logits.view(flattened_output_sequence.shape[0], -1)\n",
    "        loss = self.criterion(logits_flatten, flattened_output_sequence)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: LyricsGenerationTensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Defines the validation step - for this module, we have the same\n",
    "        training and validation behaviors. Usually, we would compute a metric that is\n",
    "        used to select the best performing model checkpoint.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : LyricsGenerationTensor\n",
    "            The batched training instances.\n",
    "        batch_idx: int\n",
    "            The index of the batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            The loss computed using CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        logits, _ = self.model(batch.input_sequence, batch.sequence_length)\n",
    "        flattened_output_sequence = batch.output_sequence.view(-1)\n",
    "        # This is needed for cross entropy loss\n",
    "        # Shape: (batch_size * sequence_length, num_classes)\n",
    "        logits_flatten = logits.view(flattened_output_sequence.shape[0], -1)\n",
    "        loss = self.criterion(logits_flatten, flattened_output_sequence)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configures optimizers for the training.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtahsTPDHf3o"
   },
   "source": [
    "The model checkpoint for this question should be named as `vanilla_rnn_model.ckpt`. Please follow the instructions to submit checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "531ac2997787411094d8c8d9e50bdb31",
      "421367481f6e4229b00c77edb94caf78",
      "d75cb59d5ee54ab6bb600da288434b9f",
      "f35c68ff30b74289a7cb1525b6e8adbc",
      "042621e73ac54a6b9b58905bdc09cf33",
      "0fc9145608e04b8384f099c653104484",
      "50f59da569d642cea1ca9094bd020a46",
      "dabf906943994ebd97a046ae8d666433",
      "cd345605781e4b6592b81c3aff4a9349",
      "52b8dc3f33d44caf82c30525d642f3c4",
      "a98f95e4488a4ba39d66f48c4a21ee55",
      "f8f7fc2d7fd9489cacdf2fd4a9a6c92a",
      "b27d914fe10b4af995415ce4ccf02452",
      "2b178f47c9aa4839a37b75a45e31c610",
      "6041ec7761ed40dd8e2dd29e88816bd9",
      "51e4e013d4684ce1944f1892ba16f297",
      "a31bfffcc063417c8ad90d8cc98902ee",
      "f26e0af8dde94eed96f2d5456d26acf7",
      "fb6c79a4e2d54b8ebe8a1c0f46f5f680",
      "627061e473a7485195c71950d32f2ce5",
      "adc53c3c27594cd48e7108d1f7107f69",
      "40f800221a184bcd868c51a0fedfb925",
      "2392960a9d1a4af1bd3bee772c553e78",
      "3efc574677034c11a2511c66b584f9f1",
      "01a17b161b724b2f804b3a8ca7710be6",
      "c3a010d2816a409aaec373ac4fba697a",
      "638e82f102d04cd58f32016af66ec155",
      "236e1976f15242d5958dd9d4b2a9fd84",
      "ff9389666d9d407f84a0c5eb4ee7ebc5",
      "d58666b3e0e24080ac7fd5a6c25280b1",
      "d59f2d1b9efa4e92a04531a2addba2ba",
      "023f127176a9457a90b15aa3f4eb0d36",
      "2632e512d6cf488792a2680bca5be98c",
      "85fc7d467eb8496cb0bdb1695d3df71b",
      "ce275746b4e34bbca620e8f55a57410c",
      "4508b4a0ee7e4f1b90b99eb25d401e60",
      "88ef14000b0a4561ba355296637debfc",
      "cc729a65f58248b5ab388010196d551e",
      "e636d489de4242b9b1412bcc07b1a0c6",
      "d69e534c08d9407eb7315716ee578612",
      "42ac4603d01944f28c69d9cca9116ce0",
      "0b9507a6685b4ce089e1614f275cd8a5",
      "c4c8272030b04ae398097d2bf2e7579f",
      "a4b2e1b9d0304b48a3167b7d2bad1c2d",
      "a43f5bef84af4cc4b1a253a810537dbf",
      "ff0a308b7f26438a9bd583bae1c1685e",
      "00c06f0134ad4653b98197cdb881e8ff",
      "5558279175094db0a66cdcaa38e34497",
      "5cf378f384b04216a8c70a24eebb4bab",
      "2673bcb258814e2aaca7654c3f9ddad8",
      "d590916368a5403bac2c3d8a00f5b08e",
      "0fed7337338c41aa811fcfcc32b2a2c0",
      "8184ce14800a4982a9c3eac11cad3836",
      "289915ec1fbd4ed281f46a578e597b96",
      "a00bff8c84a5433390efcf4ce56d0a15",
      "399a434b343e4a67a1db52d1fc5ec22e",
      "c21747fdcf304467a4c24825cc9ed339",
      "9838a6f6b2644c7b803fde0baa2a6c97",
      "02db71ba7743430a97f8f5757f3e8e69",
      "15d7674ab52e4c05a7fedce4b23cf537",
      "7a283fd2c38d40d88cb49b65c54de4c6",
      "45108bc3b6d8417985e08d14ada0c6f9",
      "cf1e9e5fd109476ab450adee00426c2a",
      "ba3bf8baece3414690fe40a834023272",
      "0b8054f99a1f40cd841d824a759c86f1",
      "ddf3b1e88ced4b849ade50f60362a245",
      "06422bd2ef344773a9bb3c9f40c922a3",
      "67b553fb1ff241bdbb392b0c1b0051be",
      "2744002ae43241dc84fb45ee1ee29f00",
      "7b3ffe79e2f045deb1e7bda4bd1b5287",
      "ac341e2beb604a0ab1ccfaad78d4b039",
      "da7851a654c9402cac6e2fa35e259a76",
      "85ddd0c8ade045cda387ac450ac81303",
      "1900262f3ef74d69ae911ecce4074b48",
      "68908a7e33b346d58dcd741050f771a2",
      "213e92311bd94f899c39a65079c7a92e",
      "b93024cdb95443e092030ed4cf4d583b",
      "eae468ab03fa41978f1c49ef856e5624",
      "49289c95e81b441cb4fd90b23dbd1424",
      "c53035c687ea4326b465de0a2fed3478",
      "4d8a429f3016434d8e4fc98281534ac9",
      "0cffa7021ad4482ab32c7bd316ba64c8",
      "23895139f67548abbe1660d785b89393",
      "e73f40b44dbf4113bf418be2b33f2ac6",
      "0b7390398c3348e3a00c09fa850056cf",
      "e8a89958914e468091d128381a9d30d3",
      "3d8f49bafac04dadbe088bed19aa7e27",
      "db489aa10c50408b8c8a29369b6953f4",
      "399889a40bdb4051a77a559ee4174eea",
      "33582ba081b74ffaa2b6673d31d465fb"
     ]
    },
    "id": "XQZAIBOSHf3o",
    "outputId": "856c0a1e-c981-42d3-884d-13e543c43dfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | VanillaRNN       | 1.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.615     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531ac2997787411094d8c8d9e50bdb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f7fc2d7fd9489cacdf2fd4a9a6c92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2392960a9d1a4af1bd3bee772c553e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fc7d467eb8496cb0bdb1695d3df71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43f5bef84af4cc4b1a253a810537dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0a308b7f26438a9bd583bae1c1685e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c06f0134ad4653b98197cdb881e8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9838a6f6b2644c7b803fde0baa2a6c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2744002ae43241dc84fb45ee1ee29f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53035c687ea4326b465de0a2fed3478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "# Training the model for 8 epochs might take about 10 mins\n",
    "vanilla_rnn = VanillaRNN()\n",
    "if is_autograder:  # The autograder would only load your trained checkpoint instead of training a new one\n",
    "    vanilla_rnn_model = LyricsGenerationTask.load_from_checkpoint(checkpoint_path=VANILLA_RNN_MODEL_CKPT_PATH,\n",
    "                                                                  model=vanilla_rnn)\n",
    "else:  # In the student mode, a new model would be trained\n",
    "    vanilla_rnn_model = LyricsGenerationTask(model=vanilla_rnn, learning_rate=0.001)\n",
    "    vanilla_rnn_trainer = pl.Trainer(accelerator=accelerator, max_epochs=8)\n",
    "    vanilla_rnn_trainer.fit(model=vanilla_rnn_model, datamodule=lyrics_gen_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxRa8xo9D0su"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XItaTvesHf3o"
   },
   "source": [
    "#### Text Generation\n",
    "\n",
    "During the text generation process we can define a starting prefix of characters that our model will condition on and generate the rest of the sequence. The process of generating sequence is called decoding. There are multiple ways to decode from encoded hidden representations: **Greedy Decoding** chooses the most probable next word for the current step, **Sampling Decoding** chooses a sample from the probability distribution defined by the logits output by the neural model, **Beam Decoding** keeps a beam of fixed number of possible candidates, etc.,. To get some basic sense of these decoding methods, we would suggest you to read the [blog post](https://towardsdatascience.com/the-three-decoding-methods-for-nlp-23ca59cb1e9d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N66QFbbsHf3p"
   },
   "source": [
    "In this section, we would like to start with a `DecodingStrategy` class and implement **Greedy Decoding**, and then extend it to **Sampling Decoding**. _(15 pts)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zCqlearnHf3p"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hypothesis:\n",
    "    \"\"\"Stores a hypothesis for the generation.\"\"\"\n",
    "    token_ids: torch.Tensor  # Shape: (batch_size, seq_len)\n",
    "    logits: torch.Tensor  # Shape: (batch_size, seq_len)\n",
    "    lengths: torch.Tensor  # Shape: (batch_size)\n",
    "\n",
    "\n",
    "class DecodingStrategy(object):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        # We have to plug-in the model here\n",
    "        # This is the `torch.nn.Module` instead of the `pl.LightningModule`\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode(self, **kwargs) -> List[Hypothesis]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "o3AFfwZ2Hf3q"
   },
   "outputs": [],
   "source": [
    "class GreedyDecodingStrategy(DecodingStrategy):\n",
    "    @torch.no_grad()\n",
    "    def decode(self,\n",
    "               starting_hyp: Hypothesis,\n",
    "               max_length: int = 300) -> List[Hypothesis]:\n",
    "        \"\"\"Decodes using greedy decoding.\n",
    "\n",
    "        Note:\n",
    "        Please make sure all tensors in the hypothesis are on the\n",
    "        same device as the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        starting_hyp : Hypothesis\n",
    "            A hypothesis contains one or more generated tokens\n",
    "            for the model to start with.\n",
    "        max_length : int, optional\n",
    "            The default is 300. Set the limit number of tokens\n",
    "            to be in the hypothesis.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hypotheses : List[Hypothesis]\n",
    "            A list of hypotheses generated by the model.\n",
    "            For GreedyDecoding, it would only generate one, but\n",
    "            keep the list type for compatibility.\n",
    "        \"\"\"\n",
    "        hypotheses: List[Hypothesis] = []\n",
    "        # Shape: (batch_size, seq_len)\n",
    "        input_sequence = starting_hyp.token_ids\n",
    "        # Shape: (batch_size)\n",
    "        sequence_lengths = torch.clone(starting_hyp.lengths)  # Makes sure not modifying the original tensor.\n",
    "        # Shape: (batch_size, seq_len)\n",
    "        logits = starting_hyp.logits\n",
    "\n",
    "        steps_generated = input_sequence.shape[-1]\n",
    "        assert steps_generated < max_length, f'steps_generated={steps_generated} >= max_length={max_length}'\n",
    "        steps_to_go: int = max_length - steps_generated + 1\n",
    "        hidden: Optional[torch.Tensor] = None\n",
    "\n",
    "        batch_size = input_sequence.shape[0]\n",
    "        #input_sequence_length shape(batch_size)\n",
    "        #input_sequence_length = torch.ones((batch_size))\n",
    "\n",
    "        # TODO: Your code here\n",
    "        for i in range(steps_to_go):\n",
    "          \n",
    "\n",
    "          #logit shape (bat_size,seq_len=1,num_class)\n",
    "          logits,hidden = self.model(input_sequence,sequence_lengths,hidden)\n",
    "\n",
    "          #num_class (batch_size)\n",
    "          arg_id = torch.argmax(logits[:,-1,:],dim=1)\n",
    "          #num_class (batch_size,1)\n",
    "          arg_id = arg_id.unsqueeze(dim=1)\n",
    "\n",
    "          #cat/append the logit into input_sequence as the next input\n",
    "          #concatenate (batch_size, seq_len) (bat_size,1)\n",
    "          input_sequence = torch.cat((input_sequence,arg_id),dim=1)\n",
    "          #logits = torch.cat((logits,logit),dim=1)\n",
    "          sequence_lengths+=1\n",
    "          \n",
    "        hypotheses.append(Hypothesis(input_sequence,logits,sequence_lengths))\n",
    "        return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnpTJvbuHf3q",
    "outputId": "be7c3f94-6ed6-4d4f-8f53-412e0959e9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iaches and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want you\n",
      "[44, 10, 12, 17, 14, 28, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30, 27, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 28, 29, 10, 27, 29, 94, 10, 23, 13, 94, 18, 94, 32, 10, 23, 29, 94, 34, 24, 30]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "vanilla_rnn = VanillaRNN()\n",
    "vanilla_rnn_model = LyricsGenerationTask.load_from_checkpoint(checkpoint_path=VANILLA_RNN_MODEL_CKPT_PATH,\n",
    "                                                                 model=vanilla_rnn)\n",
    "# Seed everything to make sure all experiments are reproducible\n",
    "#pl.seed_everything(seed=777)\n",
    "greedy_decoding = GreedyDecodingStrategy(model=vanilla_rnn_model.model)\n",
    "decoded_hypos = greedy_decoding.decode(\n",
    "    starting_hyp=Hypothesis(\n",
    "        token_ids=torch.tensor(string_to_labels('I', oov_label=OOV_LABEL), dtype=torch.long,\n",
    "                               device=vanilla_rnn_model.device).unsqueeze(0),\n",
    "        logits=torch.ones([1, 1], dtype=torch.float, device=vanilla_rnn_model.device),\n",
    "        lengths=torch.ones([1], dtype=torch.long, device=vanilla_rnn_model.device)\n",
    "    ),\n",
    ")\n",
    "print(''.join([all_characters[x] for x in decoded_hypos[0].token_ids.tolist()[0]]))\n",
    "#print(decoded_hypos[0].logits[0])\n",
    "pred_token_tuple = decoded_hypos[0].token_ids.tolist()[0]\n",
    "print(decoded_hypos[0].token_ids.tolist()[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "YlTQsyWPLy1I"
   },
   "outputs": [],
   "source": [
    "decoder = greedy_decoding\n",
    "starting_words = ['I', 'W', 'G', 'H']\n",
    "starting_hyp=Hypothesis(\n",
    "    token_ids=torch.tensor([string_to_labels(w, oov_label=OOV_LABEL) for w in starting_words], dtype=torch.long,\n",
    "                            device=vanilla_rnn_model.device),\n",
    "    logits=torch.ones([len(starting_words), 1], dtype=torch.float, device=vanilla_rnn_model.device),\n",
    "    lengths=torch.ones([len(starting_words)], dtype=torch.long, device=vanilla_rnn_model.device))\n",
    "\n",
    "mhypos = decoder.decode(starting_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2n0fWetHf3q"
   },
   "outputs": [],
   "source": [
    "grader.check(\"rnnnlm-lyric-gen-greedy-decoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqDvlB3lHf3q"
   },
   "source": [
    "The model generates each new token (characters in our case) by sampling from the output probability distribution. When sampling, we can set a `temperature` parameter that controls the randomness of the sampling process.\n",
    "\n",
    "Complete the following `sample_from_rnn` function, using the hints provided in the comments. _(10 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RACe9kQnHf3q"
   },
   "outputs": [],
   "source": [
    "class SamplingDecodingStrategy(DecodingStrategy):\n",
    "    @torch.no_grad()\n",
    "    def decode(self,\n",
    "               starting_hyp: Hypothesis,\n",
    "               max_length: int = 300,\n",
    "               temperature: float = 0.5) -> List[Hypothesis]:\n",
    "        \"\"\"Decodes using greedy decoding.\n",
    "\n",
    "        Note:\n",
    "        Please make sure all tensors in the hypothesis are on the\n",
    "        same device as the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        starting_hyp : Hypothesis\n",
    "            A hypothesis contains one or more generated tokens\n",
    "            for the model to start with.\n",
    "        max_length : int, optional\n",
    "            The default is 300. Set the limit number of tokens\n",
    "            to be in the hypothesis.\n",
    "        temperature : float, optional\n",
    "            The default is 0.5. Set the temperature for constructing\n",
    "            the categorical distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hypotheses : List[Hypothesis]\n",
    "            A list of hypotheses generated by the model.\n",
    "            For SamplingDecoding, it would only generate one, but\n",
    "            keep the list type for compatibility.\n",
    "        \"\"\"\n",
    "        epsilon: float = 1e-5  # Used to avoid zero division\n",
    "        hypotheses: List[Hypothesis] = []\n",
    "        # Shape: (batch_size, seq_len)\n",
    "        input_sequence = starting_hyp.token_ids\n",
    "        # Shape: (batch_size)\n",
    "        sequence_lengths = torch.clone(starting_hyp.lengths)  # Makes sure not modifying the original tensor.\n",
    "        # Shape: (batch_size, seq_len)\n",
    "        logits = starting_hyp.logits\n",
    "\n",
    "        steps_generated = input_sequence.shape[-1]\n",
    "        assert steps_generated < max_length, f'steps_generated={steps_generated} >= max_length={max_length}'\n",
    "        steps_to_go: int = max_length - steps_generated + 1\n",
    "        hidden: Optional[torch.Tensor] = None\n",
    "\n",
    "        # TODO: Your code here\n",
    "        batch_size = input_sequence.shape[0]\n",
    "        \n",
    "        #input_sequence_length shape(batch_size)\n",
    "        input_sequence_length = torch.ones((batch_size));\n",
    "\n",
    "        # TODO: Your code here\n",
    "        for i in range(steps_to_go):\n",
    "            #input shape(batch_size,seq_len=1)\n",
    "            #input = input_sequence[:,-1].unsqueeze(dim=1)\n",
    "\n",
    "            #logit shape (bat_size,seq_len=1,num_class)\n",
    "            logits,hidden = self.model(input_sequence,sequence_lengths,hidden)\n",
    "\n",
    "            #(batch_size)\n",
    "            sample_id = torch.distributions.categorical.Categorical(logits = logits[:,-1,:]/(temperature+epsilon)).sample()\n",
    "            #(batch_size,1)\n",
    "            sample_id = sample_id.unsqueeze(dim=1)\n",
    "            #print(sample_id.shape)\n",
    "\n",
    "            #cat/append the logit into input_sequence as the next input\n",
    "             #concatenate (batch_size, seq_len) (bat_size,1)\n",
    "            #print(input_sequence.shape,sample_id.shape)\n",
    "            input_sequence = torch.cat((input_sequence,sample_id),dim=1)\n",
    "            sequence_lengths+=1\n",
    "        hypotheses.append(Hypothesis(input_sequence,logits,sequence_lengths))\n",
    "        \n",
    "        return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBvbIAggHf3r",
    "outputId": "f9e8d9b3-81dc-4664-8cf9-805d31c01553"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 10, 12, 14, 94, 24, 15, 94, 30, 28, 94, 10, 23, 13, 94, 18, 15, 94, 18, 94, 21, 24, 31, 14, 94, 34, 24, 30, 94, 18, 94, 21, 24, 31, 14, 94, 34, 24, 30, 94, 18, 94, 17, 10, 13, 94, 10, 94, 32, 10, 29, 14, 27, 15, 10, 21, 21, 94, 24, 23, 94, 22, 34, 94, 17, 14, 10, 13, 94, 15, 24, 27, 94, 10, 94, 25, 18, 27, 29, 34, 94, 10, 23, 13, 94, 18, 68, 22, 94, 16, 24, 23, 23, 10, 94, 17, 14, 10, 27, 94, 10, 23, 13, 94, 10, 94, 13, 18, 12, 20, 94, 24, 23, 94, 29, 17, 14, 94, 17, 24, 24, 13, 94, 18, 23, 94, 29, 17, 14, 94, 28, 24, 23, 16, 94, 17, 10, 23, 13, 28, 94, 24, 23, 94, 17, 14, 27, 14, 94, 29, 24, 94, 22, 14, 14, 29, 94, 10, 94, 21, 18, 29, 29, 21, 14, 94, 15, 24, 24, 29, 94, 24, 23, 94, 22, 34, 94, 17, 14, 10, 27, 29, 94, 18, 23, 94, 29, 17, 14, 94, 17, 18, 29, 94, 29, 17, 14, 94, 15, 21, 24, 24, 27, 94, 10, 23, 13, 94, 28, 24, 94, 17, 24, 21, 13, 94, 24, 30, 29, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34, 14, 10, 17, 94, 34]\n",
      "Iace of us and if i love you i love you i had a waterfall on my head for a pirty and i'm gonna hear and a dick on the hood in the song hands on here to meet a little foot on my heart in the hit the floor and so hold out yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#vanilla_rnn = VanillaRNN()\n",
    "#vanilla_rnn_model = LyricsGenerationTask.load_from_checkpoint(checkpoint_path=VANILLA_RNN_MODEL_CKPT_PATH,\n",
    "#                                                                 model=vanilla_rnn)\n",
    "# Seed everything to make sure all experiments are reproducible\n",
    "\"\"\"\n",
    "pl.seed_everything(seed=777)\n",
    "sampling_decoding = SamplingDecodingStrategy(model=vanilla_rnn_model.model)\n",
    "sampling_decoded_hypos = sampling_decoding.decode(\n",
    "    starting_hyp=Hypothesis(\n",
    "        token_ids=torch.tensor(string_to_labels('I', oov_label=OOV_LABEL), dtype=torch.long,\n",
    "                               device=vanilla_rnn_model.device).unsqueeze(0),\n",
    "        logits=torch.zeros([1, 1], dtype=torch.float, device=vanilla_rnn_model.device),  # Log-prob\n",
    "        lengths=torch.ones([1], dtype=torch.long, device=vanilla_rnn_model.device)\n",
    "    )\n",
    ")\n",
    "pred_token_tuple = sampling_decoded_hypos[0].token_ids.tolist()[0]\n",
    "print(sampling_decoded_hypos[0].token_ids.tolist()[0])\n",
    "print(''.join([all_characters[x] for x in sampling_decoded_hypos[0].token_ids.tolist()[0]]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "94SichXtHf3r",
    "outputId": "4d5e3a5c-2c70-4c6f-82aa-748abf6a7246"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>rnnnlm-lyric-gen-sampling-decoding</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "rnnnlm-lyric-gen-sampling-decoding results: All test cases passed!"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"rnnnlm-lyric-gen-sampling-decoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuZiVtRjHf3r"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ How do you obtain probability distribution over the possible characters? _(2 points)_\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbipY7z3Hf3r"
   },
   "source": [
    "we can use softmax function on the output of RNN model, which is the logit. Softmax function can transfer the logit into probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paS2ytsCHf3r"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ How does the `temperature` affect the generated text? Describe using some values of temperature. _(3 points)_\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V2NbliVHf3r"
   },
   "source": [
    "p = exp(logit/temperature)/Z. The denominator temperature can increasing the probability of the most likely words. when temperature =1, there is no influence. However, if temperature -> 0, the probability of most likely words would approximate 1, thus turning sampling into argmax function. if temprature-> infinity, result in uniform distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHdCW5bGHf3r"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ Explain why the sequential nature of recurrent neural networks makes them less efficient compared to methods based on convolutions, for example. _(2 points)_\n",
    "\n",
    "__Answer:__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmf0FrI_Hf3r"
   },
   "source": [
    "Because RNN works in a sequential way. For a sentence with length n, we need to first compute the value at t=0, then use this information to compute t=1, then t=2, until t=n. Therefore the time complexity would be o(n). \n",
    "In contrast, in CNN, the time complexity is o(1) because there is no dependency between the inputs and we can compute all the outputs in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDiAHfqFHf3s"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ In the homework exercise we have loaded whole text sequences into the GPU memory. Imagine that we will work with sequences of much greater length, how that will this affect GPU memory consumption? How can you solve this problem? _(3 points)_\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IawT72WEHf3s"
   },
   "source": [
    "long sentence would consume too much GPU memory and then cause out of memory problem. We can solve this problem by limiting the maximum length of sentences or by dividing long sentences into short ones. we also can reduce the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTQ8E4Nk0NIs"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "In the above class, we used a vanilla RNN. Replace the RNN layers with LSTM or GRU and perform training on the same data. Does it converge faster/slower than the RNN? Try sampling from this model with the same starting string and temperature. Do you observe any difference? _(10 points)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oY6yLWqOHf3s"
   },
   "source": [
    "The improved model actually converge slower than vanilla model, and cost more time to training.\n",
    "1. sampling result from Vanilla model: \"Iaches and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want your heart in the start and i want you\"\n",
    "\n",
    "2.  sampling result from Improved model: \"IBgotta tell you some day dog down he work work work wanna hear i don't want it all over you she got that she don't wanna see a lovegame that i don't got the side of the top the world and i can see the bass work work won't you could i can see it love it don't you like it was born the shit i can be yo\"\n",
    "\n",
    "The sentences generated by vanilla model shows lots of repeat, while the one generated by improved model got huge improve and become more grammatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZ7QWmW20WOC"
   },
   "outputs": [],
   "source": [
    "class ImprovedSeqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int = 101,\n",
    "                 hidden_size: int = 512,\n",
    "                 num_classes: int = 100,\n",
    "                 n_layers: int = 2,\n",
    "                 padding_label: int = PADDING_LABEL):\n",
    "        # input_size = 101 -- 100 characters + background character\n",
    "        # num_classes = 100 -- we predict what character goes next\n",
    "        super(ImprovedSeqModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.padding_label = padding_label\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Your code here\n",
    "        # Please include necessary changes to this class\n",
    "        # You can set args to be default\n",
    "        self.embeddings = nn.Embedding(self.input_size, self.hidden_size, self.padding_label)        \n",
    "\n",
    "        # Below use nn.LSTM that accepts hidden_size as input size,\n",
    "        # and has hidden size equal to hidden_size argument and n_layers\n",
    "        # You have to set `batch_first=True` to make it compatible with\n",
    "        # the `forward()` method.\n",
    "        # TODO: Your code here\n",
    "\n",
    "        self.lstm = nn.LSTM(self.hidden_size,self.hidden_size , self.n_layers,batch_first=True)\n",
    "\n",
    "        # Below use nn.Linear to make representation of hidden_size\n",
    "        # to the number of classes that will be fed to softmax\n",
    "        # to decide which character goes next\n",
    "        # TODO: Your code here\n",
    "        self.ffnn = nn.Linear(self.hidden_size,self.num_classes)\n",
    "\n",
    "    def forward(self,\n",
    "                input_sequences: torch.Tensor,\n",
    "                input_sequences_lengths: torch.Tensor,\n",
    "                hidden: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Runs the forward pass for the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sequences : torch.Tensor\n",
    "            A batch of padded input sequences in the shape of\n",
    "            (batch_size, sequence_length)\n",
    "        input_sequences_lengths : torch.Tensor\n",
    "            A batch of sequence lengths in the shape of (batch_size)\n",
    "        hidden : torch.Tensor, optional\n",
    "            A batch of hidden states in the shape of\n",
    "            (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits_flatten : torch.Tensor\n",
    "            A batch of logits in the shape of\n",
    "            (batch_size, sequence_length, num_classes)\n",
    "        updated_hidden : torch.Tensor\n",
    "            A batch of updated hidden states in the shape of\n",
    "            (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        \"\"\"\n",
    "        # Please make sure your code follow the above interface\n",
    "        # We would use your implemented decoding strategy to decode\n",
    "        # Your code here\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        embedded = self.embeddings(input_sequences)\n",
    "\n",
    "        # This is needed for efficient processing of sequences of\n",
    "        # variable lengths. Feel free to skip\n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        packed: PackedSequence = torch.nn.utils.rnn.pack_padded_sequence(embedded,\n",
    "                                                                         input_sequences_lengths.cpu(),\n",
    "                                                                         batch_first=True,\n",
    "                                                                         enforce_sorted=False)\n",
    "\n",
    "        # `outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        # `updated_hidden` shape: (num_layers, batch_size, hidden_size)\n",
    "        outputs, updated_hidden = self.lstm(packed, hidden)\n",
    "        # `unpacked_outputs` shape: (batch_size, sequence_length, hidden_size)\n",
    "        # `output_lengths` shape: (batch_size)\n",
    "        # Unpacks (back to padded)\n",
    "        unpacked_outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs,\n",
    "                                                                                  batch_first=True)\n",
    "\n",
    "        # Shape: (batch_size, sequence_length, num_classes)\n",
    "        logits = self.ffnn(unpacked_outputs)\n",
    "        \n",
    "        return logits, updated_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "NpiDIuwaHf3s",
    "outputId": "b75d6197-d942-4854-c445-b5b071374f97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-80cf7fbaa853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# But you are not allowed to create a new task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimproved_model_pl_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLyricsGenerationTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimproved_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimproved_model_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mimproved_model_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimproved_model_pl_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlyrics_gen_data_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mamp_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamp_backend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mamp_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamp_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0mplugins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         )\n\u001b[1;32m    452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoggerConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_gpu_accelerator_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_parallel_devices_and_init_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# 3. Instantiate ClusterEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_devices_flag_if_auto_select_gpus_passed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parallel_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/cuda.py\u001b[0m in \u001b[0;36mparse_devices\u001b[0;34m(devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;34m\"\"\"Accelerator device parsing logic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36mparse_gpu_ids\u001b[0;34m(gpus, include_cuda, include_mps)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0m_check_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_sanitize_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_mps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36m_sanitize_gpu_ids\u001b[0;34m(gpus, include_cuda, include_mps)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one gpu type should be specified!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mall_available_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_all_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_mps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_available_gpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36m_get_all_available_gpus\u001b[0;34m(include_cuda, include_mps)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \"\"\"\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mcuda_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_all_available_cuda_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minclude_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0mmps_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_all_available_mps_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minclude_mps\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcuda_gpus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmps_gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36m_get_all_available_cuda_gpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m          \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cuda_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36mnum_cuda_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fork\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    223\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helping task handler/workers to finish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_help_stuff_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# task_handler may be blocked trying to put items on inqueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'removing tasks from inqueue until task handler finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model for 8 epochs might take about 10 mins\n",
    "improved_model = ImprovedSeqModel()  # You can modify this to accommodate your model hyperparameters\n",
    "if is_autograder:  # The autograder would only load your trained checkpoint instead of training a new one\n",
    "    improved_model_pl_module = LyricsGenerationTask.load_from_checkpoint(checkpoint_path=IMPROVED_MODEL_CKPT_PATH,\n",
    "                                                                         model=improved_model)\n",
    "else:  # In the student mode, a new model would be trained\n",
    "    # You are allowed to change training hyperparameters\n",
    "    # But you are not allowed to create a new task\n",
    "    improved_model_pl_module = LyricsGenerationTask(model=improved_model, learning_rate=0.001)\n",
    "    improved_model_trainer = pl.Trainer(accelerator=accelerator, max_epochs=8)\n",
    "    improved_model_trainer.fit(model=improved_model_pl_module, datamodule=lyrics_gen_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "oh11isV7Hf3s",
    "outputId": "4305a0bf-a3b8-47a8-e88c-cefc5709876d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>rnnnlm-lyric-gen-improved-model</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "rnnnlm-lyric-gen-improved-model results: All test cases passed!"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"rnnnlm-lyric-gen-improved-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H417M1dvuZzU",
    "outputId": "02dee3df-64ec-4d2b-904c-933a4f8b90c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBgotta tell you some day dog down he work work work wanna hear i don't want it all over you she got that she don't wanna see a lovegame that i don't got the side of the top the world and i can see the bass work work won't you could i can see it love it don't you like it was born the shit i can be yo\n"
     ]
    }
   ],
   "source": [
    "sampling_decoding = SamplingDecodingStrategy(model=improved_model_pl_module.model)\n",
    "sampling_decoded_hypos = sampling_decoding.decode(\n",
    "    starting_hyp=Hypothesis(\n",
    "        token_ids=torch.tensor(string_to_labels('I', oov_label=OOV_LABEL), dtype=torch.long,\n",
    "                               device=vanilla_rnn_model.device).unsqueeze(0),\n",
    "        logits=torch.zeros([1, 1], dtype=torch.float, device=vanilla_rnn_model.device),  # Log-prob\n",
    "        lengths=torch.ones([1], dtype=torch.long, device=vanilla_rnn_model.device)\n",
    "    )\n",
    ")\n",
    "print(''.join([all_characters[x] for x in sampling_decoded_hypos[0].token_ids.tolist()[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZw9_-l8jLCU"
   },
   "source": [
    "### Part 2: Distributional Semantics: The Word2Vec model\n",
    "\n",
    "In Part 2 of this assignment, you will use Word2Vec to analyze some of interesting phenomena that arises from distributional semantics. For this, we will use a pretrained model from the [Gensim](https://radimrehurek.com/gensim/) package.\n",
    "\n",
    "\n",
    "Before we dive into this analysis, first read a little bit of theory about the Word2Vec model and answer a few questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDaseIPkANWm"
   },
   "source": [
    "#### Understanding word2vec\n",
    "\n",
    "The basic architecture of the skip-gram model is shown below (taken from [this post](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)):\n",
    "\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/skip_gram_net_arch.png)\n",
    "\n",
    "In this model, word embeddings are trained using a classification task, where the task is that given a word, you have to predict its context words (i.e. words which occur before and after it in a context of size, say, 2). The objective is to learn the probability of any word $O$ given a center word $C$, i.e. $P(O=o \\mid C=c)$.\n",
    "\n",
    "In this model, this probability is computed by taking the vector dot product of the embeddings for $o$ and $c$, and then applying a softmax (to convert it into a probability distribution):\n",
    "\n",
    "$$\n",
    "P(O=o \\mid C=c)=\\frac{\\exp \\left(\\boldsymbol{v}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)}{\\sum_{w \\in \\operatorname{Vocab}} \\exp \\left(\\boldsymbol{v}_{w}^{\\top} \\boldsymbol{v}_{c}\\right)}\n",
    "$$\n",
    "\n",
    "The vectors $\\boldsymbol{v}_{w}$ are initialized randomly for all the words in the vocabulary and then updated during training. The objective, in fact, tries to maximize the above probability of words in context occuring together. Stating it different, it miniimized the negative log-probability, i.e.,\n",
    "\n",
    "$$ J = -\\log P(O=o \\mid C=c). $$\n",
    "\n",
    "Here is another interpretation of this objective function. Suppose ${y}$ and $\\hat{y}$ are vectors of length $|V|$ (size of vocabulary), and the index $k$ in each of these denotes the conditional probability of word $k$ being in the context of given word $c$. $y$ is the empirical distribution (ground truth), i.e., it is a one-hot vector, whereas $\\hat{y}$ is the predicted distribution. In our case, we would have $P(O=o \\mid C=c) = \\hat{y}_o$. Then the loss function $J$ above is equivalent to the binary cross-entropy loss between $y$ and $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krmPnh86Hf3t"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ The cross-entropy loss between 2 distributions is given as $-\\sum_i p_i \\log q_i$. With this information, show that the cross-entropy between $y$ and $\\hat{y}$ is equal to $-\\log \\hat{y}_o$. _(5 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLQcywe4Hf3t"
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "550UwNKWHf3t"
   },
   "source": [
    "The cross-entropy between $y$ and $\\hat{y}$: \n",
    "$$H(y,\\hat{y})=-\\sum_i y_i \\log \\hat{y}_i$$\n",
    "Beacause $y$ is a one-hot vector, therefore,\\\\\n",
    "1. If i is o, then $y_o \\log \\hat{y}_o=\\log \\hat{y}_o$\n",
    "2. If i is not o, then $y_i=0$, thus $y_i \\log \\hat{y}_i=0$\n",
    "\n",
    "Thus, \n",
    "$$H(y,\\hat{y})=-\\sum_i y_i \\log \\hat{y}_i=-(0+y_o \\log \\hat{y}_o)=-\\log\\hat{y}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh6D5LUmJnyk"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Note that in the softmax computation, the denominator computes a sum over the entire vocabulary. This is very computationally expensive. In the actual word2vec implementation, we use something called Negative Sampling instead of computing the softmax this way:\n",
    "\n",
    "$$J^{\\prime} = -\\log \\sigma(v_o v_c) - \\sum_{k=1}^K \\log \\sigma(-v_k v_c)$$\n",
    "\n",
    "Here, $v_k$ is the embedding for a word which is not present in the context. Intuitively, this loss function pushes $v_c$ closer to its context word $v_o$ while pulling it away from randomly sampled $K$ non-context (or negative) words, hence the name negative sampling.\n",
    "\n",
    "An important component of the above objective is the sigmoid function $\\sigma(x) = \\frac{e^x}{e^x + 1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wrnE70yHf3t"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ Compute the derivative of $\\sigma(x)$ w.r.t $x$. Write the derivative in terms of $\\sigma(x)$. (Show the derivation steps.) _(5 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWseEFYGHf3t"
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-FQxEtfHf3t"
   },
   "source": [
    "the derivative of sigmoid equation equals to $\\sigma(x)(1-\\sigma(x))$.\n",
    "\\begin{align}\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}x}\\sigma(x) &= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\frac{e^{x}}{e^{x}+1} \\\\\n",
    "&= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\frac{1}{1+e^{-x}} \\\\\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^{2}} \\\\\n",
    "&= \\frac{1}{1+e^{-x}}\\cdot \\frac{e^{-x}}{1+e^{-x}} \\\\\n",
    "&= \\sigma(x)(1-\\sigma(x))\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXAvtTl5COQC"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "$\\sigma(x)$ has the nice property that its derivative can be represented using it directly. This makes it useful for gradient-based optimization. In the following, we will use the `gensim` implementation of Word2Vec, which means you don't need to implement any of the training yourself, but it is important to understand what goes on under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GRWFeCRkcFP"
   },
   "source": [
    "#### Using word2vec\n",
    "\n",
    "In this section, we will use the GenSim implementation of Word2Vec for some exploration. First, let us train a Word2Vec model on our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsEmWXPAhBDL"
   },
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "    Return:\n",
    "        wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    wv_from_bin = gensim_api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(wv_from_bin.index_to_key))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eInUSXOOhIcP"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take several minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()\n",
    "print(wv_from_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxBXR4oOcKDk"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Embedding analysis:__ Choose 20 words at random from the lexicon and obtain their corresponding embeddings. Reduce the embedding dimensionality to 2 (using PCA or T-SNE from scikit-learn) and plot them. Do you observe any clusters forming? _(10 points)_\n",
    "\n",
    "Note: for scikit-learn, please refer to [its documentation](https://scikit-learn.org/stable/user_guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f383hVjoc_aw"
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cseuUOIHf3u"
   },
   "source": [
    "yes, some words are grouped together, which means they share similare meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "720NHAADHf3u",
    "outputId": "7a17a462-f2f2-476e-f513-d79a07dd7aa1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lexicons = list(wv_from_bin.key_to_index.keys())\n",
    "words = np.random.choice(lexicons,20)\n",
    "embeddings = np.array([ wv_from_bin[w] for w in words]) # 20*200\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embd = pca.fit_transform(embeddings)\n",
    "#print(reduced_embd.shape)\n",
    "plt.scatter(reduced_embd[:,0],reduced_embd[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwsBypXPd5q4",
    "outputId": "ba6243b1-7034-43c2-8df3-58d10afa657b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.4426336e-02 -2.9442110e+00 -4.6552342e-01  6.1606851e+00\n",
      " -5.3906494e-01  2.3794246e-01 -2.5458428e-01  2.1324627e-01\n",
      "  1.0575556e+00  8.9165103e-01  2.1537908e-04  1.4175439e-01\n",
      " -5.7919168e-01 -6.5273935e-01 -2.3205793e+00 -7.3639047e-01\n",
      "  3.0240753e-01  5.4314733e-01  8.0922860e-01 -1.9499778e+00]\n"
     ]
    }
   ],
   "source": [
    "print(reduced_embd[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1mn9cK7ef3H"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Synonyms and antonyms:__ Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
    "\n",
    "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
    "\n",
    "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. _(5 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ozeiFzRHf3u",
    "outputId": "4eab149c-6c31-49e1-ecb4-83e1518351de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2947447896003723 0.3333911895751953\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "w1=\"hard\"\n",
    "w2 = \"easy\"\n",
    "w3 =\"tough\"\n",
    "d12 = wv_from_bin.distance(w1, w2)\n",
    "d13 = wv_from_bin.distance(w1, w3)\n",
    "print(d13,d12)\n",
    "#explanation: because hard and easy are usually come together as neighbour in training corpus. while tough may be less connected with hard in courpus. \n",
    "#That why hard and easy have more strong relationship than hard and tough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8MVeoWJdlfi"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "__Finding analogies:__ Implement a function `analogy()` which solves the analogy problem, e.g., `man:king::woman:x`. Use the `most_similar()` function provided in Gensim for your implementation. _(3 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkUrl7oFx5Fr"
   },
   "outputs": [],
   "source": [
    "def analogy(x1: str, x2: str, y1: str, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Solves for x1:x2::y1:?\n",
    "    Returns the top-k similar ones.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    result = wv_from_bin.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hECnbiTekXkU",
    "outputId": "74293769-dcd4-4402-a7c8-ed1f3b4cd28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6978678107261658), ('princess', 0.6081745028495789), ('monarch', 0.5889754891395569), ('throne', 0.5775108933448792), ('prince', 0.5750998258590698), ('elizabeth', 0.5463595986366272), ('daughter', 0.5399126410484314), ('kingdom', 0.5318052768707275), ('mother', 0.5168544054031372), ('crown', 0.5164473056793213)]\n"
     ]
    }
   ],
   "source": [
    "result = wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "u1Slo9xBHf3v",
    "outputId": "74923ec9-18a3-4064-c645-6cc3c0c733da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>word2vec-analogy-impl</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "word2vec-analogy-impl results: All test cases passed!"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"word2vec-analogy-impl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWmmYZheHf3v"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Analyzing bias:__ Complete the following analogies: `man:worker::woman:x` and `woman:worker::man:x` and list the most likely outputs in both cases. Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias. _(5 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LW8imhAHf3v"
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdfcIFVFHf3v"
   },
   "source": [
    "The result show that woman workers are usually relate to nurse, mother, teacher, homemaker, while man workers have strong relationship to employee, laborer, mechanic, factory.\n",
    "This can reflect the gender stereotypes in our language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZlVAbS7Hf3v",
    "outputId": "e4f82643-27d6-4ff2-aeb3-e2709bc2485d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('employee', 0.6375863552093506), ('workers', 0.6068920493125916), ('nurse', 0.5837947130203247), ('pregnant', 0.5363885164260864), ('mother', 0.5321308970451355), ('employer', 0.5127025842666626), ('teacher', 0.5099576711654663), ('child', 0.5096741318702698), ('homemaker', 0.5019454956054688), ('nurses', 0.4970572590827942)]\n",
      "[('workers', 0.611325740814209), ('employee', 0.5983108878135681), ('working', 0.5615329742431641), ('laborer', 0.5442320108413696), ('unemployed', 0.536851704120636), ('job', 0.5278826355934143), ('work', 0.5223963856697083), ('mechanic', 0.5088937282562256), ('worked', 0.5054520964622498), ('factory', 0.4940454363822937)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "ret1 = analogy(\"man\", \"worker\", \"woman\", 10)\n",
    "print(ret1)\n",
    "ret2 = analogy(\"woman\", \"worker\", \"man\", 10)\n",
    "print(ret2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCIepIbDHf3v"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "__Question:__ Suggest some ways to deal with the gender (or racial) bias when training word embedding models, or when using pre-trained models. _(5 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTArhKCLHf3v"
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ04rgM9Hf3w"
   },
   "source": [
    "we can manully generate some sentences which can relate man with nurse, homemaker, teacher, and relate woman with factory,mechanic, and laborer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORo_-koKHf3w"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c06f0134ad4653b98197cdb881e8ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5558279175094db0a66cdcaa38e34497",
       "IPY_MODEL_5cf378f384b04216a8c70a24eebb4bab",
       "IPY_MODEL_2673bcb258814e2aaca7654c3f9ddad8"
      ],
      "layout": "IPY_MODEL_d590916368a5403bac2c3d8a00f5b08e"
     }
    },
    "01a17b161b724b2f804b3a8ca7710be6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d58666b3e0e24080ac7fd5a6c25280b1",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d59f2d1b9efa4e92a04531a2addba2ba",
      "value": 28
     }
    },
    "023f127176a9457a90b15aa3f4eb0d36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02db71ba7743430a97f8f5757f3e8e69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf1e9e5fd109476ab450adee00426c2a",
      "placeholder": "​",
      "style": "IPY_MODEL_ba3bf8baece3414690fe40a834023272",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "042621e73ac54a6b9b58905bdc09cf33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "06422bd2ef344773a9bb3c9f40c922a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b7390398c3348e3a00c09fa850056cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b8054f99a1f40cd841d824a759c86f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b9507a6685b4ce089e1614f275cd8a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0cffa7021ad4482ab32c7bd316ba64c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d8f49bafac04dadbe088bed19aa7e27",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db489aa10c50408b8c8a29369b6953f4",
      "value": 28
     }
    },
    "0fc9145608e04b8384f099c653104484": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fed7337338c41aa811fcfcc32b2a2c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15d7674ab52e4c05a7fedce4b23cf537": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b8054f99a1f40cd841d824a759c86f1",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ddf3b1e88ced4b849ade50f60362a245",
      "value": 28
     }
    },
    "1900262f3ef74d69ae911ecce4074b48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "213e92311bd94f899c39a65079c7a92e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "236e1976f15242d5958dd9d4b2a9fd84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23895139f67548abbe1660d785b89393": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_399889a40bdb4051a77a559ee4174eea",
      "placeholder": "​",
      "style": "IPY_MODEL_33582ba081b74ffaa2b6673d31d465fb",
      "value": " 28/28 [00:05&lt;00:00,  5.51it/s]"
     }
    },
    "2392960a9d1a4af1bd3bee772c553e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3efc574677034c11a2511c66b584f9f1",
       "IPY_MODEL_01a17b161b724b2f804b3a8ca7710be6",
       "IPY_MODEL_c3a010d2816a409aaec373ac4fba697a"
      ],
      "layout": "IPY_MODEL_638e82f102d04cd58f32016af66ec155"
     }
    },
    "2632e512d6cf488792a2680bca5be98c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2673bcb258814e2aaca7654c3f9ddad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_399a434b343e4a67a1db52d1fc5ec22e",
      "placeholder": "​",
      "style": "IPY_MODEL_c21747fdcf304467a4c24825cc9ed339",
      "value": " 28/28 [00:05&lt;00:00,  5.47it/s]"
     }
    },
    "2744002ae43241dc84fb45ee1ee29f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b3ffe79e2f045deb1e7bda4bd1b5287",
       "IPY_MODEL_ac341e2beb604a0ab1ccfaad78d4b039",
       "IPY_MODEL_da7851a654c9402cac6e2fa35e259a76"
      ],
      "layout": "IPY_MODEL_85ddd0c8ade045cda387ac450ac81303"
     }
    },
    "289915ec1fbd4ed281f46a578e597b96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b178f47c9aa4839a37b75a45e31c610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb6c79a4e2d54b8ebe8a1c0f46f5f680",
      "max": 161,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_627061e473a7485195c71950d32f2ce5",
      "value": 161
     }
    },
    "33582ba081b74ffaa2b6673d31d465fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "399889a40bdb4051a77a559ee4174eea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "399a434b343e4a67a1db52d1fc5ec22e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d8f49bafac04dadbe088bed19aa7e27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3efc574677034c11a2511c66b584f9f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_236e1976f15242d5958dd9d4b2a9fd84",
      "placeholder": "​",
      "style": "IPY_MODEL_ff9389666d9d407f84a0c5eb4ee7ebc5",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "40f800221a184bcd868c51a0fedfb925": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "421367481f6e4229b00c77edb94caf78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fc9145608e04b8384f099c653104484",
      "placeholder": "​",
      "style": "IPY_MODEL_50f59da569d642cea1ca9094bd020a46",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "42ac4603d01944f28c69d9cca9116ce0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4508b4a0ee7e4f1b90b99eb25d401e60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42ac4603d01944f28c69d9cca9116ce0",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b9507a6685b4ce089e1614f275cd8a5",
      "value": 0
     }
    },
    "45108bc3b6d8417985e08d14ada0c6f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "49289c95e81b441cb4fd90b23dbd1424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d8a429f3016434d8e4fc98281534ac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b7390398c3348e3a00c09fa850056cf",
      "placeholder": "​",
      "style": "IPY_MODEL_e8a89958914e468091d128381a9d30d3",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "50f59da569d642cea1ca9094bd020a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51e4e013d4684ce1944f1892ba16f297": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "52b8dc3f33d44caf82c30525d642f3c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "531ac2997787411094d8c8d9e50bdb31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_421367481f6e4229b00c77edb94caf78",
       "IPY_MODEL_d75cb59d5ee54ab6bb600da288434b9f",
       "IPY_MODEL_f35c68ff30b74289a7cb1525b6e8adbc"
      ],
      "layout": "IPY_MODEL_042621e73ac54a6b9b58905bdc09cf33"
     }
    },
    "5558279175094db0a66cdcaa38e34497": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fed7337338c41aa811fcfcc32b2a2c0",
      "placeholder": "​",
      "style": "IPY_MODEL_8184ce14800a4982a9c3eac11cad3836",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "5cf378f384b04216a8c70a24eebb4bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_289915ec1fbd4ed281f46a578e597b96",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a00bff8c84a5433390efcf4ce56d0a15",
      "value": 28
     }
    },
    "6041ec7761ed40dd8e2dd29e88816bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc53c3c27594cd48e7108d1f7107f69",
      "placeholder": "​",
      "style": "IPY_MODEL_40f800221a184bcd868c51a0fedfb925",
      "value": " 161/161 [01:33&lt;00:00,  1.72it/s, loss=1.29, v_num=0]"
     }
    },
    "627061e473a7485195c71950d32f2ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "638e82f102d04cd58f32016af66ec155": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "67b553fb1ff241bdbb392b0c1b0051be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68908a7e33b346d58dcd741050f771a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a283fd2c38d40d88cb49b65c54de4c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06422bd2ef344773a9bb3c9f40c922a3",
      "placeholder": "​",
      "style": "IPY_MODEL_67b553fb1ff241bdbb392b0c1b0051be",
      "value": " 28/28 [00:05&lt;00:00,  5.53it/s]"
     }
    },
    "7b3ffe79e2f045deb1e7bda4bd1b5287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1900262f3ef74d69ae911ecce4074b48",
      "placeholder": "​",
      "style": "IPY_MODEL_68908a7e33b346d58dcd741050f771a2",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "8184ce14800a4982a9c3eac11cad3836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85ddd0c8ade045cda387ac450ac81303": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "85fc7d467eb8496cb0bdb1695d3df71b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce275746b4e34bbca620e8f55a57410c",
       "IPY_MODEL_4508b4a0ee7e4f1b90b99eb25d401e60",
       "IPY_MODEL_88ef14000b0a4561ba355296637debfc"
      ],
      "layout": "IPY_MODEL_cc729a65f58248b5ab388010196d551e"
     }
    },
    "88ef14000b0a4561ba355296637debfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4c8272030b04ae398097d2bf2e7579f",
      "placeholder": "​",
      "style": "IPY_MODEL_a4b2e1b9d0304b48a3167b7d2bad1c2d",
      "value": " 0/28 [00:00&lt;?, ?it/s]"
     }
    },
    "9838a6f6b2644c7b803fde0baa2a6c97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02db71ba7743430a97f8f5757f3e8e69",
       "IPY_MODEL_15d7674ab52e4c05a7fedce4b23cf537",
       "IPY_MODEL_7a283fd2c38d40d88cb49b65c54de4c6"
      ],
      "layout": "IPY_MODEL_45108bc3b6d8417985e08d14ada0c6f9"
     }
    },
    "a00bff8c84a5433390efcf4ce56d0a15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a31bfffcc063417c8ad90d8cc98902ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4b2e1b9d0304b48a3167b7d2bad1c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a98f95e4488a4ba39d66f48c4a21ee55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac341e2beb604a0ab1ccfaad78d4b039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_213e92311bd94f899c39a65079c7a92e",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b93024cdb95443e092030ed4cf4d583b",
      "value": 28
     }
    },
    "adc53c3c27594cd48e7108d1f7107f69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b27d914fe10b4af995415ce4ccf02452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a31bfffcc063417c8ad90d8cc98902ee",
      "placeholder": "​",
      "style": "IPY_MODEL_f26e0af8dde94eed96f2d5456d26acf7",
      "value": "Epoch 7: 100%"
     }
    },
    "b93024cdb95443e092030ed4cf4d583b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba3bf8baece3414690fe40a834023272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c21747fdcf304467a4c24825cc9ed339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3a010d2816a409aaec373ac4fba697a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_023f127176a9457a90b15aa3f4eb0d36",
      "placeholder": "​",
      "style": "IPY_MODEL_2632e512d6cf488792a2680bca5be98c",
      "value": " 28/28 [00:05&lt;00:00,  5.56it/s]"
     }
    },
    "c4c8272030b04ae398097d2bf2e7579f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c53035c687ea4326b465de0a2fed3478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d8a429f3016434d8e4fc98281534ac9",
       "IPY_MODEL_0cffa7021ad4482ab32c7bd316ba64c8",
       "IPY_MODEL_23895139f67548abbe1660d785b89393"
      ],
      "layout": "IPY_MODEL_e73f40b44dbf4113bf418be2b33f2ac6"
     }
    },
    "cc729a65f58248b5ab388010196d551e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "cd345605781e4b6592b81c3aff4a9349": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce275746b4e34bbca620e8f55a57410c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e636d489de4242b9b1412bcc07b1a0c6",
      "placeholder": "​",
      "style": "IPY_MODEL_d69e534c08d9407eb7315716ee578612",
      "value": "Validation DataLoader 0:   0%"
     }
    },
    "cf1e9e5fd109476ab450adee00426c2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d58666b3e0e24080ac7fd5a6c25280b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d590916368a5403bac2c3d8a00f5b08e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "d59f2d1b9efa4e92a04531a2addba2ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d69e534c08d9407eb7315716ee578612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d75cb59d5ee54ab6bb600da288434b9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dabf906943994ebd97a046ae8d666433",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd345605781e4b6592b81c3aff4a9349",
      "value": 2
     }
    },
    "da7851a654c9402cac6e2fa35e259a76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eae468ab03fa41978f1c49ef856e5624",
      "placeholder": "​",
      "style": "IPY_MODEL_49289c95e81b441cb4fd90b23dbd1424",
      "value": " 28/28 [00:05&lt;00:00,  5.55it/s]"
     }
    },
    "dabf906943994ebd97a046ae8d666433": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db489aa10c50408b8c8a29369b6953f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ddf3b1e88ced4b849ade50f60362a245": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e636d489de4242b9b1412bcc07b1a0c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e73f40b44dbf4113bf418be2b33f2ac6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "e8a89958914e468091d128381a9d30d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eae468ab03fa41978f1c49ef856e5624": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f26e0af8dde94eed96f2d5456d26acf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f35c68ff30b74289a7cb1525b6e8adbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52b8dc3f33d44caf82c30525d642f3c4",
      "placeholder": "​",
      "style": "IPY_MODEL_a98f95e4488a4ba39d66f48c4a21ee55",
      "value": " 2/2 [00:02&lt;00:00,  1.00s/it]"
     }
    },
    "f8f7fc2d7fd9489cacdf2fd4a9a6c92a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b27d914fe10b4af995415ce4ccf02452",
       "IPY_MODEL_2b178f47c9aa4839a37b75a45e31c610",
       "IPY_MODEL_6041ec7761ed40dd8e2dd29e88816bd9"
      ],
      "layout": "IPY_MODEL_51e4e013d4684ce1944f1892ba16f297"
     }
    },
    "fb6c79a4e2d54b8ebe8a1c0f46f5f680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff9389666d9d407f84a0c5eb4ee7ebc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
